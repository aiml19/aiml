{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Details\n",
    "\n",
    "**Batch**                -  May 2019 Batch (Group 10 A)  \n",
    "**Project Type**         -  Capstone project  \n",
    "**Project Domain**       -  NLP  \n",
    "**Project Name**         -  Automatic Ticket Assignment  \n",
    "**Submission Date**      -  17-May-2020      \n",
    "**Submitted By**         -  Group10A  \n",
    "**Delivery Type**        -  Milestone 2,3\n",
    "\n",
    "## The Real Problem\n",
    "One of the key activities of any IT function is to “Keep the lights on” to ensure there is noimpact to the Business operations. IT leverages Incident Management process to achieve theabove Objective. An incident is something that is unplanned interruption  to  an  IT  service  orreduction  in  the quality of an IT service that affects the Users and theBusiness. The main goal of Incident Management process is to provide a quick fix / workarounds or solutions thatresolves the interruption and restores the service to its full capacity to ensure no businessimpact.In most of the organizations, incidents are created by various Business and IT Users, End Users/ Vendors if they have access to ticketing systems, and from the integrated monitoringsystems and tools. Assigning the incidents to the appropriate person or unit in the support team has critical importance to provide improved user satisfaction while ensuring better allocation of support resources. The assignment of incidents to appropriate IT \n",
    "groups is still a manual process in many of the IT organizations.Manual assignment of incidents is time consuming and requires human efforts. There may bemistakes due to human errors and resource consumption is carried out ineffectively because of the misaddressing. On the other hand, manual assignment increases the response and resolution times which result in user satisfaction deterioration / poor customer service.\n",
    "\n",
    "# Business Domain Value\n",
    "In the support process,incoming incidents are analyzed and assessed by organization’s support teams to fulfill the request. \n",
    "In many organizations, better allocation and effective usage of the valuable support resources will directly result \n",
    "in substantial cost savings.Currently the incidents are created by various stakeholders (Business Users, IT Users \n",
    "and Monitoring Tools) within IT Service Management Tool and are assigned to Service Desk teams (L1 / L2 teams). This team will review the incidents for right ticket categorization, priorities and then carry out initial diagnosis to see if they can resolve. Around ~54% of the incidents are resolved by L1 / L2 teams. Incase L1 / L2 is unable to resolve, they will then escalate / assign the tickets to Functional teams from Applications and Infrastructure (L3 teams). Some portions of incidents are directly assigned to L3 teams by either Monitoring tools or Callers / Requestors. L3 teams will carry out detailed diagnosis and resolve the incidents. Around ~56% \n",
    "of incidents are resolved by Functional / L3 teams. Incase if vendor support is needed, they will reach out for their support towards incident closure.L1 / L2 needs to spend time reviewing Standard Operating Procedures (SOPs) before assigning to Functional teams (Minimum ~25-30% of incidents needs to be reviewed for SOPs before ticket assignment). 15 min is being spent for SOP review for each incident. Minimum of ~1 FTE effort needed only for incident assignment to L3 teams.During the process of incident assignments by L1 / L2 teams to functional groups, there were multiple instances of incidents getting assigned to wrong functional groups. Around ~25% of Incidents are wrongly assigned to functional teams. Additional effort needed for Functional teams to re-assign to right functional groups. During this process, some of the incidents are in queue and not addressed timely resulting in poor customer service.Guided by powerful AI techniques that can classify incidents to right functional groups can help organizations to reduce the resolving time of the issue and can focus on more productive tasks.\n",
    "\n",
    "# Project Description\n",
    "\n",
    "In this capstone project, the goal is to build a classifier that can classify the tickets by analysing text.Details about the data and dataset files are given in below link,https://drive.google.com/file/d/1OZNJm81JXucV3HmZroMq6qCT2m7ez7IJ \n",
    "\n",
    "## Milestone 1: \n",
    "● Pre-Processing, Data Visualisation and EDA Overview  \n",
    "● Exploring the given Data files  \n",
    "● Understanding the structure of data  \n",
    "● Missing points in data  \n",
    "● Finding inconsistencies in the data  \n",
    "● Visualizing different patterns   \n",
    "● Visualizing different text features  \n",
    "● Dealing with missing values  \n",
    "● Text preprocessing   \n",
    "● Creating word vocabulary from the corpus of report text data  \n",
    "● Creating tokens as required   \n",
    "\n",
    "## Milestone 2\n",
    "● Model BuildingOverview  \n",
    "● Building a model architecture which can classify.  \n",
    "● Trying different model architectures by researching state of the art for similar tasks.  \n",
    "● Train the model  \n",
    "● To deal with large training time, save the weights so that you can use them when training the model for the second time without starting from scratch.  \n",
    "\n",
    "## Milestone 3:  \n",
    "● Test the Model, Fine-tuning and RepeatOverview.  \n",
    "● Test the model and report as per evaluation metrics.  \n",
    "● Try different models.  \n",
    "● Try different evaluation metrics.   \n",
    "● Set different hyper parameters, by trying different optimizers, loss functions, epochs, learning rate, batch size, checkpointing, early stopping etc..for these models to fine-tune them.  \n",
    "● Report evaluation metrics for these models along with your observation on how changing different hyper parameters leads to change in the final evaluation metric.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:24 : importing modules\n",
      "17:33:24 : completed importing modules\n",
      "17:33:24 : loading functions\n",
      "17:33:24 : Imported modules and new functions completed\n"
     ]
    }
   ],
   "source": [
    "# # Let us first start with the date to note down timing\n",
    "# import warnings\n",
    "# warnings.filterwarnings('always')\n",
    "\n",
    "from datetime import datetime\n",
    "def time_now():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "def print_msg(*msg):\n",
    "    print(time_now(),\":\",*msg)\n",
    "\n",
    "## Import all the required modules\n",
    "print_msg(\"importing modules\")\n",
    "\n",
    "## most used packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re,os,io,json\n",
    "\n",
    "### NLP packages\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## Keras packages\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer,tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input ,Flatten,BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding,GlobalMaxPool1D,Bidirectional,SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "\n",
    "# sklearn packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier   \n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\n",
    "from sklearn import metrics,preprocessing,svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "print_msg(\"completed importing modules\")\n",
    "\n",
    "print_msg(\"loading functions\")\n",
    "          \n",
    "## Functions to loading data, build models and data preprocessing\n",
    "## New Log table to capture metrics from different model iterations \n",
    "try:                              \n",
    "    len(log)\n",
    "except:\n",
    "    print(\"create log table\")\n",
    "    log_cols = [\"groups\",\"model_name\",\"model_column\",\"data_set\", \"Accuracy\",\"Precision Score\",\"Recall Score\",\"F1-Score\",\"kappa_score\"]\n",
    "    log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "# Function to capture metrics in the log table, save table to log.xlsx file with every run\n",
    "def metric_update(y_test,y_pred):\n",
    "    global model_column,model_name,data_set\n",
    "    global log,itr_cnt\n",
    "    accuracy = metrics.accuracy_score(y_test,y_pred)\n",
    "    precision = metrics.precision_score(y_test,y_pred,average='macro',labels=np.unique(y_pred))\n",
    "    recall = metrics.recall_score(y_test,y_pred,average='macro',labels=np.unique(y_pred))\n",
    "    f1_score = metrics.f1_score(y_test,y_pred,average='macro',labels=np.unique(y_pred))\n",
    "    kappa_score=metrics.cohen_kappa_score(y_test,y_pred)\n",
    "    col_data=[msg_grp,model_name,model_column,data_set,accuracy,precision,recall,f1_score,kappa_score]\n",
    "    log_entry = pd.DataFrame([col_data], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    itr_cnt=itr_cnt+1\n",
    "    print_msg(\"completed iteration ={}\".format(itr_cnt))\n",
    "    print_msg(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "\n",
    "# Function to load data and perform preprocessing,add new columns in dataframe    \n",
    "def load_data(file,prefix):        \n",
    "    print_msg(\"loading file=\",file)\n",
    "    df = pd.read_excel(file)\n",
    "    df= df.drop(\"Caller\" , axis=1)\n",
    "    df.columns=[\"short_description\",\"long_description\",\"assigned_group\"]\n",
    "    df[\"combined_description\"]=df[\"short_description\"]+\" \"+df[\"long_description\"]\n",
    "    df.dropna(inplace=True) ## Not many null ,so can safely drop the rows\n",
    "    df,name=preprocess_column(df,\"short_description\")\n",
    "    df,name=preprocess_column(df,\"combined_description\")\n",
    "    df[\"assigned_group_org\"]=df[\"assigned_group\"]\n",
    "    df.dropna(subset=['combined_description_text'],inplace=True)\n",
    "    df.short_description_text[df.short_description_text.isnull()]=df.combined_description_text[df.short_description_text.isnull()]\n",
    "    df.to_excel(prefix+file)\n",
    "    return df\n",
    "    \n",
    "\n",
    "# Data preprocessing \n",
    "# When we set the flag deacc=True , the function removes punctuations also \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True , min_len=3, max_len=20))  # deacc=True removes punctuations\n",
    "\n",
    "# covert list to sentences\n",
    "def words_to_sent(sentences):\n",
    "    final=[]\n",
    "    for sentence in sentences:\n",
    "        local=\" \".join(sentence)\n",
    "        final.append(local)\n",
    "    return final\n",
    "\n",
    "## Stopword ,duplicate word removal\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'received'])\n",
    "\n",
    "def remove_stopwords_duplicate(texts):\n",
    "    final=[]\n",
    "    for doc in texts:\n",
    "        local=[]\n",
    "        for word in simple_preprocess(str(doc)):\n",
    "            if word not in local:\n",
    "                local.append(word)\n",
    "        final.append(local)\n",
    "    return final\n",
    "\n",
    "## find POS for each word for lemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN,  \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_document(documents):\n",
    "    ## Convert to lower case\n",
    "    documents = [sent.lower() for sent in documents]\n",
    "    # Remove Emails\n",
    "    documents = [re.sub('\\S*@\\S*\\s?', ' ', sent) for sent in documents]\n",
    "    # Remove new line characters\n",
    "    documents = [re.sub('\\s+', ' ', sent) for sent in documents]\n",
    "    # Remove _\n",
    "    documents = [re.sub('_', ' ', sent) for sent in documents]\n",
    "    # Remove Numbers\n",
    "    documents = [re.sub('\\d+', ' ', sent) for sent in documents]\n",
    "    # Remove  distracting single quotes\n",
    "    documents = [re.sub('\\'', ' ', sent) for sent in documents]\n",
    "    # Remove all non word characters\n",
    "    documents = [re.sub('\\W', ' ', sent) for sent in documents]\n",
    "    \n",
    "    document_words = list(sent_to_words(documents))\n",
    "    document_words = remove_stopwords_duplicate(document_words)\n",
    "    \n",
    "    # Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "#     print_msg(\"lemmatization started\")\n",
    "    hl_lemmatized = []\n",
    "    for tokens in document_words:\n",
    "        lemm = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
    "        hl_lemmatized.append(lemm)\n",
    "    document_words=hl_lemmatized   \n",
    "#     print_msg(\"lemmatization ended\")\n",
    "    \n",
    "    return document_words\n",
    "\n",
    "def preprocess_column(df,column_name):\n",
    "    new_column=column_name+\"_list\"\n",
    "    new_column1=column_name+\"_text\"\n",
    "    try:\n",
    "        df[new_column].shape\n",
    "        print_msg(\"pre-processing was already done\")\n",
    "    except:\n",
    "        print_msg(\"process started for \"+column_name)\n",
    "        documents = df[column_name].values.tolist()\n",
    "        document_words=preprocess_document(documents)\n",
    "        df[new_column]=document_words\n",
    "        df[new_column1]=words_to_sent(document_words)\n",
    "        print_msg(\"process finished for \"+column_name)\n",
    "    return df,new_column\n",
    "\n",
    "## sklearn models with default parameters\n",
    "def sklearn_model(column_name):\n",
    "    global model_column\n",
    "    global model_name\n",
    "    global data_set\n",
    "    print_msg(\"fitting all classic models with column_name=\",column_name)\n",
    "    model_column=column_name\n",
    "    \n",
    "    X = df[model_column]\n",
    "    y = df.assigned_group.astype('category')\n",
    " \n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(X)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_tfidf = tfidf_transformer.fit_transform(X_bow)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model_name = \"LogisticRegression\"\n",
    "    sklearn_model_fit(model,X_bow,X_tfidf,y)\n",
    "\n",
    "    model = DecisionTreeClassifier(criterion = 'entropy' )\n",
    "    model_name =\"DecisionTreeClassifier\"\n",
    "    sklearn_model_fit(model,X_bow,X_tfidf,y)\n",
    "\n",
    "    model= RandomForestClassifier()\n",
    "    model_name = \"RandomForestClassifier\"\n",
    "    sklearn_model_fit(model,X_bow,X_tfidf,y)\n",
    "\n",
    "    model = AdaBoostClassifier(n_estimators= 20)\n",
    "    model_name = \"AdaBoostClassifier\"\n",
    "    sklearn_model_fit(model,X_bow,X_tfidf,y)\n",
    "\n",
    "    model = BaggingClassifier()\n",
    "    model_name = \"BaggingClassifier\"\n",
    "    sklearn_model_fit(model,X_bow,X_tfidf,y)\n",
    "   \n",
    "#     model = GradientBoostingClassifier()\n",
    "#     name = \"GradientBoostingClassifier\"\n",
    "#     classic_model_fit(name,model,X_bow,X_tfidf,y)\n",
    "\n",
    "    model = GaussianNB()\n",
    "    X_array_bow  = X_bow.toarray()\n",
    "    X_array_tfidf  = X_tfidf.toarray()\n",
    "    model_name = \"GaussianNB\"\n",
    "    sklearn_model_fit(model,X_array_bow,X_array_tfidf,y)\n",
    "\n",
    "    model = svm.SVC()\n",
    "    model_name = \"svm.svc\"\n",
    "    sklearn_model_fit(model,X_bow,X_tfidf,y)\n",
    "    \n",
    "def sklearn_model_fit(model,X_bow,X_tfidf,y):\n",
    "    global model_column,model_name,data_set\n",
    "    data_set=\"Bow\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_bow,y, test_size=0.10, random_state=42,stratify=y)\n",
    "    print_msg(\"working on\",msg_grp,model_column,model_name,data_set)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    metric_update(y_test,y_pred)\n",
    "    \n",
    "    data_set=\"TFIDF\"\n",
    "    print_msg(\"working on\",msg_grp,model_column,model_name,data_set)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf,y, test_size=0.10, random_state=42,stratify=y)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    metric_update(y_test,y_pred) \n",
    "    \n",
    "try:\n",
    "    len(embeddings)\n",
    "except:\n",
    "    embeddings = {}\n",
    "    \n",
    "def embedding(glove_file):\n",
    "    global embeddings\n",
    "    \n",
    "    if len(embeddings) < 1000:\n",
    "        print_msg(\"embedding length\",len(embeddings))\n",
    "        print_msg(\"Building Embeddings from \"+glove_file)\n",
    "\n",
    "        for o in open(glove_file,encoding=\"utf8\"):\n",
    "            word = o.split(\" \")[0]\n",
    "            # print_msg(word)\n",
    "            embd = o.split(\" \")[1:]\n",
    "            embd = np.asarray(embd, dtype='float32')\n",
    "            # print_msg(embd)\n",
    "            embeddings[word] = embd\n",
    "    else:\n",
    "        print_msg(\"Embeddings from \",glove_file,\"already exists\")\n",
    "    print_msg(\"No. of embeddings = \", len(embeddings))  \n",
    "\n",
    "def keras_model(column_name,epoch=5,fit=0):\n",
    "    global model_column,model_name,data_set\n",
    "    model_column=column_name\n",
    "    model_name=\"Keras LSTM Model\"\n",
    "    data_set=\"processed\"\n",
    "    \n",
    "    print_msg(\"working on\",msg_grp,model_column,model_name,data_set)\n",
    "    \n",
    "    max_features = 10000\n",
    "    #epoch = 20\n",
    "    batch_size = 100\n",
    "    \n",
    "    documents = df[model_column].values.tolist()\n",
    "    max_allowed=max(df[\"short_description_text\"].apply(lambda x: len(x.split(\" \"))))*2\n",
    "    max_allowed=40\n",
    "    maxlen = max(df[model_column].apply(lambda x: len(x)))\n",
    "    #print_msg(\"maxlen before=\",maxlen) \n",
    "    if maxlen>max_allowed:\n",
    "        maxlen=max_allowed\n",
    "    print_msg(\"maxlen after=\",maxlen)    \n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(documents))\n",
    "    sequence = tokenizer.texts_to_sequences(documents)\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        \n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    print_msg(\"My vocabulary size = \",vocab_size)\n",
    "    X = pad_sequences(sequence, maxlen = maxlen, padding='post',truncating='post') \n",
    "    \n",
    "    groups = list(df.assigned_group.unique())\n",
    "    with open('group.txt', 'w') as filehandle:\n",
    "        for listitem in groups:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(groups)\n",
    "    y=to_categorical(le.transform(df.assigned_group))   \n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1, random_state = 42,stratify=y)   \n",
    "    \n",
    "    glove_file='glove.6B.200d.txt'\n",
    "    embedding(glove_file)\n",
    "    embedding_size=embeddings['the'].shape[0]\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, weights = [embedding_matrix],input_length=maxlen))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(df.assigned_group.nunique()*2, return_sequences = True,recurrent_dropout=0.1, dropout=0.1)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(df.assigned_group.nunique()*2, activation=\"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(df.assigned_group.nunique(), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['acc'])\n",
    "    print_msg(model.summary())\n",
    "    \n",
    "    if fit>0:\n",
    "        print_msg('running model.fit')\n",
    "        history = model.fit(X_train, y_train, epochs = epoch, batch_size=batch_size, validation_split=0.05,shuffle= True,verbose = 1)\n",
    "        model.save('my_model.h5')\n",
    "        y_predict=model.predict_classes(X_test)\n",
    "        y_test1=np.argmax(y_test,axis=1)\n",
    "        \n",
    "        metric_update(y_test1, y_predict)\n",
    "        \n",
    "def model_load():\n",
    "    with open('tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    model = load_model('my_model.h5')\n",
    "    \n",
    "    groups = []\n",
    "    with open('group.txt', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            currentPlace = line[:-1]\n",
    "            groups.append(currentPlace)\n",
    "        \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(groups)\n",
    "    \n",
    "    return le,tokenizer,model\n",
    "\n",
    "def model_predict(text):\n",
    "    document=[text]\n",
    "    documents=preprocess_document(document)\n",
    "#     print_msg(documents)\n",
    "    sequence = tokenizer.texts_to_sequences(documents)\n",
    "    maxlen=model.input.shape[1]\n",
    "    X = pad_sequences(sequence, maxlen = maxlen, padding='post',truncating='post')\n",
    "#     print_msg(X)\n",
    "    y=model.predict_classes(X)\n",
    "    print_msg(\"text=\",document,\"       predicted_class=\",le.inverse_transform(y))\n",
    "\n",
    "print_msg(\"Imported modules and new functions completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"1657data.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\work\\\\capstone\")\n",
    "file=\"1657data.xlsx\"\n",
    "df=pd.read_excel(file)\n",
    "df.short_description_list=list(sent_to_words(df.short_description_text.values.tolist()))\n",
    "df.combined_description_list=list(sent_to_words(df.combined_description_text.values.tolist()))\n",
    "# df=load_data(\"data.xlsx\",\"1657\")   # load data and backup processed data into a file prefix with 1657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:31 : total count for GRP_74=69\n",
      "17:33:31 : fitting all classic models with column_name= short_description_text\n",
      "17:33:31 : working on grp_74 with <2 rows, total groups=69 short_description_text LogisticRegression Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:32 : completed iteration =1\n",
      "17:33:32 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.72      0.96      0.82       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.71      0.36      0.48        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.65      0.65      0.65        26\n",
      "      GRP_13       0.42      0.33      0.37        15\n",
      "      GRP_14       0.40      0.33      0.36        12\n",
      "      GRP_15       1.00      0.25      0.40         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.50      0.33      0.40         9\n",
      "      GRP_19       0.50      0.14      0.21        22\n",
      "       GRP_2       0.55      0.25      0.34        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.82      0.93      0.87        29\n",
      "      GRP_25       0.60      0.50      0.55        12\n",
      "      GRP_26       0.67      0.33      0.44         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       1.00      0.25      0.40         4\n",
      "      GRP_29       0.62      0.50      0.56        10\n",
      "       GRP_3       0.56      0.25      0.34        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.50      0.29      0.36         7\n",
      "      GRP_33       1.00      0.36      0.53        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       1.00      1.00      1.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.50      0.20      0.29        10\n",
      "      GRP_40       1.00      0.40      0.57         5\n",
      "      GRP_41       1.00      0.50      0.67         4\n",
      "      GRP_42       1.00      0.25      0.40         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       1.00      0.50      0.67         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.75      0.46      0.57        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       1.00      1.00      1.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.67      0.22      0.33        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       1.00      0.43      0.60         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.55      0.89      0.68        66\n",
      "       GRP_9       0.86      0.24      0.38        25\n",
      "\n",
      "    accuracy                           0.68       849\n",
      "   macro avg       0.40      0.25      0.29       849\n",
      "weighted avg       0.64      0.68      0.62       849\n",
      "\n",
      "17:33:32 : working on grp_74 with <2 rows, total groups=69 short_description_text LogisticRegression TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:34 : completed iteration =2\n",
      "17:33:34 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.62      0.99      0.76       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.80      0.29      0.42        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.69      0.42      0.52        26\n",
      "      GRP_13       0.44      0.27      0.33        15\n",
      "      GRP_14       0.80      0.33      0.47        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       1.00      0.11      0.20         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.33      0.08      0.13        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       1.00      0.50      0.67         2\n",
      "      GRP_24       0.89      0.83      0.86        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.67      0.20      0.31        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       1.00      0.09      0.17        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.00      0.00      0.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         5\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       1.00      0.50      0.67         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.75      0.46      0.57        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.80      0.22      0.35        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       1.00      0.29      0.44         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.56      0.92      0.70        66\n",
      "       GRP_9       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.63       849\n",
      "   macro avg       0.25      0.14      0.16       849\n",
      "weighted avg       0.54      0.63      0.53       849\n",
      "\n",
      "17:33:34 : working on grp_74 with <2 rows, total groups=69 short_description_text DecisionTreeClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:34 : completed iteration =3\n",
      "17:33:35 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.69      0.83      0.76       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.40      0.43      0.41        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.42      0.54      0.47        26\n",
      "      GRP_13       0.10      0.07      0.08        15\n",
      "      GRP_14       0.50      0.25      0.33        12\n",
      "      GRP_15       0.50      0.25      0.33         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.50      0.22      0.31         9\n",
      "      GRP_19       0.31      0.23      0.26        22\n",
      "       GRP_2       0.38      0.33      0.36        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.90      0.90      0.90        29\n",
      "      GRP_25       0.45      0.42      0.43        12\n",
      "      GRP_26       0.25      0.17      0.20         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.25      0.25      0.25         4\n",
      "      GRP_29       0.57      0.40      0.47        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.40      0.50      0.44         4\n",
      "      GRP_31       0.33      0.14      0.20         7\n",
      "      GRP_33       0.36      0.36      0.36        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       1.00      1.00      1.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.27      0.30      0.29        10\n",
      "      GRP_40       0.67      0.40      0.50         5\n",
      "      GRP_41       0.33      0.25      0.29         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       1.00      0.50      0.67         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.58      0.54      0.56        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.40      0.22      0.29        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_63       0.00      0.00      0.00         0\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "      GRP_68       0.00      0.00      0.00         0\n",
      "       GRP_7       0.50      0.14      0.22         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.52      0.74      0.61        66\n",
      "       GRP_9       0.60      0.24      0.34        25\n",
      "\n",
      "    accuracy                           0.59       849\n",
      "   macro avg       0.24      0.20      0.21       849\n",
      "weighted avg       0.54      0.59      0.55       849\n",
      "\n",
      "17:33:35 : working on grp_74 with <2 rows, total groups=69 short_description_text DecisionTreeClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:36 : completed iteration =4\n",
      "17:33:36 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.72      0.84      0.78       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.50      0.36      0.42        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.52      0.54      0.53        26\n",
      "      GRP_13       0.12      0.13      0.12        15\n",
      "      GRP_14       0.40      0.33      0.36        12\n",
      "      GRP_15       1.00      0.25      0.40         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.22      0.22      0.22         9\n",
      "      GRP_19       0.10      0.09      0.10        22\n",
      "       GRP_2       0.44      0.29      0.35        24\n",
      "      GRP_20       0.33      0.25      0.29         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.75      0.72      0.74        29\n",
      "      GRP_25       0.56      0.42      0.48        12\n",
      "      GRP_26       0.50      0.33      0.40         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.29      0.50      0.36         4\n",
      "      GRP_29       0.43      0.30      0.35        10\n",
      "       GRP_3       0.12      0.10      0.11        20\n",
      "      GRP_30       0.40      0.50      0.44         4\n",
      "      GRP_31       0.25      0.14      0.18         7\n",
      "      GRP_33       0.29      0.18      0.22        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.50      1.00      0.67         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.43      0.30      0.35        10\n",
      "      GRP_40       0.50      0.20      0.29         5\n",
      "      GRP_41       0.25      0.25      0.25         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_43       0.00      0.00      0.00         0\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       0.50      0.50      0.50         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.62      0.62      0.62        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_58       0.00      0.00      0.00         0\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.56      0.28      0.37        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_63       0.00      0.00      0.00         0\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "      GRP_66       0.00      0.00      0.00         0\n",
      "       GRP_7       0.33      0.14      0.20         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.53      0.79      0.63        66\n",
      "       GRP_9       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.59       849\n",
      "   macro avg       0.23      0.20      0.20       849\n",
      "weighted avg       0.55      0.59      0.56       849\n",
      "\n",
      "17:33:36 : working on grp_74 with <2 rows, total groups=69 short_description_text RandomForestClassifier Bow\n",
      "17:33:37 : completed iteration =5\n",
      "17:33:37 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.68      0.94      0.79       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.44      0.29      0.35        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.53      0.62      0.57        26\n",
      "      GRP_13       0.38      0.33      0.36        15\n",
      "      GRP_14       0.67      0.33      0.44        12\n",
      "      GRP_15       1.00      0.50      0.67         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.50      0.22      0.31         9\n",
      "      GRP_19       0.33      0.14      0.19        22\n",
      "       GRP_2       0.36      0.17      0.23        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       1.00      0.79      0.88        29\n",
      "      GRP_25       0.75      0.25      0.38        12\n",
      "      GRP_26       1.00      0.17      0.29         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.50      0.25      0.33         4\n",
      "      GRP_29       0.62      0.50      0.56        10\n",
      "       GRP_3       0.25      0.10      0.14        20\n",
      "      GRP_30       0.40      0.50      0.44         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.56      0.45      0.50        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       1.00      1.00      1.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       1.00      0.30      0.46        10\n",
      "      GRP_40       0.00      0.00      0.00         5\n",
      "      GRP_41       1.00      0.50      0.67         4\n",
      "      GRP_42       0.50      0.25      0.33         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       1.00      0.50      0.67         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.64      0.54      0.58        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       1.00      1.00      1.00         1\n",
      "      GRP_59       1.00      1.00      1.00         1\n",
      "       GRP_6       0.67      0.22      0.33        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       1.00      0.14      0.25         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.56      0.83      0.67        66\n",
      "       GRP_9       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.65       849\n",
      "   macro avg       0.38      0.25      0.28       849\n",
      "weighted avg       0.60      0.65      0.59       849\n",
      "\n",
      "17:33:37 : working on grp_74 with <2 rows, total groups=69 short_description_text RandomForestClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:39 : completed iteration =6\n",
      "17:33:39 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.68      0.93      0.78       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.57      0.29      0.38        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.61      0.65      0.63        26\n",
      "      GRP_13       0.31      0.27      0.29        15\n",
      "      GRP_14       0.67      0.33      0.44        12\n",
      "      GRP_15       0.25      0.25      0.25         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.40      0.22      0.29         9\n",
      "      GRP_19       0.43      0.14      0.21        22\n",
      "       GRP_2       0.43      0.12      0.19        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.93      0.90      0.91        29\n",
      "      GRP_25       1.00      0.25      0.40        12\n",
      "      GRP_26       0.50      0.17      0.25         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.33      0.25      0.29         4\n",
      "      GRP_29       0.33      0.10      0.15        10\n",
      "       GRP_3       0.25      0.10      0.14        20\n",
      "      GRP_30       0.40      0.50      0.44         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.57      0.36      0.44        11\n",
      "      GRP_34       1.00      0.33      0.50         6\n",
      "      GRP_36       1.00      1.00      1.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.40      0.20      0.27        10\n",
      "      GRP_40       1.00      0.20      0.33         5\n",
      "      GRP_41       0.25      0.25      0.25         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       1.00      0.50      0.67         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.60      0.46      0.52        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_56       0.00      0.00      0.00         0\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.55      0.33      0.41        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       0.75      0.43      0.55         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.56      0.85      0.67        66\n",
      "       GRP_9       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.64       849\n",
      "   macro avg       0.31      0.20      0.23       849\n",
      "weighted avg       0.58      0.64      0.58       849\n",
      "\n",
      "17:33:39 : working on grp_74 with <2 rows, total groups=69 short_description_text AdaBoostClassifier Bow\n",
      "17:33:40 : completed iteration =7\n",
      "17:33:40 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.53      1.00      0.69       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        15\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.00      0.00      0.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         5\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       0.00      0.00      0.00         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       0.00      0.00      0.00         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.40      0.61      0.48        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.51       849\n",
      "   macro avg       0.02      0.03      0.02       849\n",
      "weighted avg       0.28      0.51      0.36       849\n",
      "\n",
      "17:33:40 : working on grp_74 with <2 rows, total groups=69 short_description_text AdaBoostClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:42 : completed iteration =8\n",
      "17:33:42 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.53      1.00      0.69       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        15\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.00      0.00      0.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         5\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       0.00      0.00      0.00         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       0.00      0.00      0.00         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.40      0.61      0.48        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.51       849\n",
      "   macro avg       0.02      0.03      0.02       849\n",
      "weighted avg       0.28      0.51      0.36       849\n",
      "\n",
      "17:33:42 : working on grp_74 with <2 rows, total groups=69 short_description_text BaggingClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:46 : completed iteration =9\n",
      "17:33:46 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.71      0.90      0.79       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.33      0.29      0.31        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.57      0.65      0.61        26\n",
      "      GRP_13       0.40      0.27      0.32        15\n",
      "      GRP_14       0.43      0.25      0.32        12\n",
      "      GRP_15       0.33      0.50      0.40         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.40      0.22      0.29         9\n",
      "      GRP_19       0.33      0.23      0.27        22\n",
      "       GRP_2       0.40      0.25      0.31        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.79      0.79      0.79        29\n",
      "      GRP_25       0.64      0.58      0.61        12\n",
      "      GRP_26       0.67      0.33      0.44         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.20      0.25      0.22         4\n",
      "      GRP_29       0.71      0.50      0.59        10\n",
      "       GRP_3       0.17      0.10      0.12        20\n",
      "      GRP_30       0.33      0.25      0.29         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.83      0.45      0.59        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.33      1.00      0.50         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.50      0.20      0.29        10\n",
      "      GRP_40       1.00      0.20      0.33         5\n",
      "      GRP_41       1.00      0.25      0.40         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       1.00      0.50      0.67         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.64      0.54      0.58        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       1.00      1.00      1.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.50      0.28      0.36        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       0.50      0.14      0.22         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.55      0.82      0.65        66\n",
      "       GRP_9       0.70      0.28      0.40        25\n",
      "\n",
      "    accuracy                           0.63       849\n",
      "   macro avg       0.30      0.23      0.24       849\n",
      "weighted avg       0.58      0.63      0.58       849\n",
      "\n",
      "17:33:46 : working on grp_74 with <2 rows, total groups=69 short_description_text BaggingClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:53 : completed iteration =10\n",
      "17:33:53 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.70      0.93      0.80       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.50      0.29      0.36        14\n",
      "      GRP_11       1.00      0.33      0.50         3\n",
      "      GRP_12       0.59      0.73      0.66        26\n",
      "      GRP_13       0.29      0.27      0.28        15\n",
      "      GRP_14       0.50      0.33      0.40        12\n",
      "      GRP_15       0.67      0.50      0.57         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.30      0.33      0.32         9\n",
      "      GRP_19       0.31      0.23      0.26        22\n",
      "       GRP_2       0.44      0.17      0.24        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       1.00      0.33      0.50         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.74      0.79      0.77        29\n",
      "      GRP_25       0.75      0.25      0.38        12\n",
      "      GRP_26       0.67      0.33      0.44         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       1.00      0.25      0.40         4\n",
      "      GRP_29       0.67      0.60      0.63        10\n",
      "       GRP_3       0.12      0.05      0.07        20\n",
      "      GRP_30       0.40      0.50      0.44         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.60      0.27      0.37        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.50      1.00      0.67         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.33      0.20      0.25        10\n",
      "      GRP_40       0.75      0.60      0.67         5\n",
      "      GRP_41       1.00      0.25      0.40         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       1.00      0.50      0.67         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.56      0.38      0.45        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       1.00      1.00      1.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.80      0.22      0.35        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       0.75      0.43      0.55         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.52      0.80      0.63        66\n",
      "       GRP_9       0.70      0.28      0.40        25\n",
      "\n",
      "    accuracy                           0.64       849\n",
      "   macro avg       0.36      0.25      0.27       849\n",
      "weighted avg       0.59      0.64      0.59       849\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:33:53 : working on grp_74 with <2 rows, total groups=69 short_description_text GaussianNB Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:34:00 : completed iteration =11\n",
      "17:34:00 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.62      0.34      0.43       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.62      0.36      0.45        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.50      0.38      0.43        26\n",
      "      GRP_13       0.44      0.47      0.45        15\n",
      "      GRP_14       0.33      0.42      0.37        12\n",
      "      GRP_15       0.29      0.50      0.36         4\n",
      "      GRP_16       0.14      0.22      0.17         9\n",
      "      GRP_17       0.88      0.88      0.88         8\n",
      "      GRP_18       0.33      0.22      0.27         9\n",
      "      GRP_19       0.13      0.18      0.15        22\n",
      "       GRP_2       0.14      0.25      0.18        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.14      0.33      0.20         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.78      0.72      0.75        29\n",
      "      GRP_25       0.14      0.08      0.11        12\n",
      "      GRP_26       0.13      0.33      0.19         6\n",
      "      GRP_27       0.05      0.50      0.09         2\n",
      "      GRP_28       0.11      0.25      0.15         4\n",
      "      GRP_29       0.33      0.40      0.36        10\n",
      "       GRP_3       0.21      0.25      0.23        20\n",
      "      GRP_30       0.40      0.50      0.44         4\n",
      "      GRP_31       0.04      0.14      0.06         7\n",
      "      GRP_33       0.44      0.36      0.40        11\n",
      "      GRP_34       0.07      0.17      0.10         6\n",
      "      GRP_36       0.50      1.00      0.67         1\n",
      "      GRP_37       0.33      0.50      0.40         2\n",
      "      GRP_39       0.20      0.50      0.29         2\n",
      "       GRP_4       0.27      0.60      0.37        10\n",
      "      GRP_40       0.12      0.40      0.19         5\n",
      "      GRP_41       0.38      0.75      0.50         4\n",
      "      GRP_42       0.25      0.50      0.33         4\n",
      "      GRP_43       0.00      0.00      0.00         0\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       0.25      0.50      0.33         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.25      0.46      0.32        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_57       0.00      0.00      0.00         0\n",
      "      GRP_58       0.00      0.00      0.00         0\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.50      0.17      0.25        18\n",
      "      GRP_60       0.20      0.50      0.29         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_63       0.00      0.00      0.00         0\n",
      "      GRP_65       0.25      1.00      0.40         1\n",
      "       GRP_7       0.20      0.14      0.17         7\n",
      "      GRP_72       0.00      0.00      0.00         0\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.74      0.26      0.38        66\n",
      "       GRP_9       0.60      0.24      0.34        25\n",
      "\n",
      "    accuracy                           0.33       849\n",
      "   macro avg       0.20      0.26      0.20       849\n",
      "weighted avg       0.50      0.33      0.37       849\n",
      "\n",
      "17:34:00 : working on grp_74 with <2 rows, total groups=69 short_description_text GaussianNB TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:34:07 : completed iteration =12\n",
      "17:34:07 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.62      0.34      0.44       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.56      0.36      0.43        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.40      0.38      0.39        26\n",
      "      GRP_13       0.50      0.47      0.48        15\n",
      "      GRP_14       0.38      0.42      0.40        12\n",
      "      GRP_15       0.29      0.50      0.36         4\n",
      "      GRP_16       0.14      0.22      0.17         9\n",
      "      GRP_17       0.88      0.88      0.88         8\n",
      "      GRP_18       0.40      0.22      0.29         9\n",
      "      GRP_19       0.19      0.27      0.23        22\n",
      "       GRP_2       0.14      0.25      0.18        24\n",
      "      GRP_20       0.10      0.25      0.14         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.17      0.33      0.22         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.84      0.72      0.78        29\n",
      "      GRP_25       0.10      0.08      0.09        12\n",
      "      GRP_26       0.13      0.33      0.19         6\n",
      "      GRP_27       0.05      0.50      0.10         2\n",
      "      GRP_28       0.10      0.25      0.14         4\n",
      "      GRP_29       0.31      0.40      0.35        10\n",
      "       GRP_3       0.15      0.20      0.17        20\n",
      "      GRP_30       0.25      0.25      0.25         4\n",
      "      GRP_31       0.04      0.14      0.07         7\n",
      "      GRP_33       0.44      0.36      0.40        11\n",
      "      GRP_34       0.13      0.33      0.19         6\n",
      "      GRP_36       0.50      1.00      0.67         1\n",
      "      GRP_37       0.50      0.50      0.50         2\n",
      "      GRP_39       0.20      0.50      0.29         2\n",
      "       GRP_4       0.27      0.60      0.37        10\n",
      "      GRP_40       0.12      0.40      0.19         5\n",
      "      GRP_41       0.38      0.75      0.50         4\n",
      "      GRP_42       0.29      0.50      0.36         4\n",
      "      GRP_43       0.00      0.00      0.00         0\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       0.25      0.50      0.33         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.26      0.46      0.33        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_57       0.00      0.00      0.00         0\n",
      "      GRP_58       0.00      0.00      0.00         0\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.60      0.17      0.26        18\n",
      "      GRP_60       0.25      0.50      0.33         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_63       0.00      0.00      0.00         0\n",
      "      GRP_65       0.25      1.00      0.40         1\n",
      "       GRP_7       0.20      0.14      0.17         7\n",
      "      GRP_72       0.00      0.00      0.00         0\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.74      0.26      0.38        66\n",
      "       GRP_9       0.56      0.20      0.29        25\n",
      "\n",
      "    accuracy                           0.33       849\n",
      "   macro avg       0.21      0.26      0.21       849\n",
      "weighted avg       0.50      0.33      0.38       849\n",
      "\n",
      "17:34:07 : working on grp_74 with <2 rows, total groups=69 short_description_text svm.svc Bow\n",
      "17:34:14 : completed iteration =13\n",
      "17:34:14 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.47      1.00      0.64       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        15\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.00      0.00      0.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         5\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       0.00      0.00      0.00         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       0.00      0.00      0.00         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.00      0.00      0.00        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.47       849\n",
      "   macro avg       0.01      0.02      0.01       849\n",
      "weighted avg       0.22      0.47      0.30       849\n",
      "\n",
      "17:34:14 : working on grp_74 with <2 rows, total groups=69 short_description_text svm.svc TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:34:19 : completed iteration =14\n",
      "17:34:19 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.47      1.00      0.64       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        15\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.00      0.00      0.00         9\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_21       0.00      0.00      0.00         3\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_23       0.00      0.00      0.00         2\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_27       0.00      0.00      0.00         2\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "      GRP_36       0.00      0.00      0.00         1\n",
      "      GRP_37       0.00      0.00      0.00         2\n",
      "      GRP_39       0.00      0.00      0.00         2\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         5\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_44       0.00      0.00      0.00         1\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "      GRP_46       0.00      0.00      0.00         1\n",
      "      GRP_47       0.00      0.00      0.00         3\n",
      "      GRP_48       0.00      0.00      0.00         2\n",
      "      GRP_49       0.00      0.00      0.00         1\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "      GRP_50       0.00      0.00      0.00         1\n",
      "      GRP_51       0.00      0.00      0.00         1\n",
      "      GRP_52       0.00      0.00      0.00         1\n",
      "      GRP_53       0.00      0.00      0.00         1\n",
      "      GRP_55       0.00      0.00      0.00         1\n",
      "      GRP_59       0.00      0.00      0.00         1\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "      GRP_60       0.00      0.00      0.00         2\n",
      "      GRP_62       0.00      0.00      0.00         2\n",
      "      GRP_65       0.00      0.00      0.00         1\n",
      "       GRP_7       0.00      0.00      0.00         7\n",
      "      GRP_74       0.00      0.00      0.00         1\n",
      "       GRP_8       0.00      0.00      0.00        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.47       849\n",
      "   macro avg       0.01      0.02      0.01       849\n",
      "weighted avg       0.22      0.47      0.30       849\n",
      "\n",
      "17:34:19"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : working on grp_74 with <2 rows, total groups=69 short_description_list Keras LSTM Model processed\n",
      "17:34:19 : maxlen after= 40\n",
      "17:34:19 : My vocabulary size =  5258\n",
      "17:34:20 : Embeddings from  glove.6B.200d.txt already exists\n",
      "17:34:20 : No. of embeddings =  400000\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 200)           1051600   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 40, 200)           800       \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 40, 276)           374256    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 276)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 276)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 276)               1104      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 138)               38226     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 138)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 138)               552       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 69)                9591      \n",
      "=================================================================\n",
      "Total params: 1,476,129\n",
      "Trainable params: 1,474,901\n",
      "Non-trainable params: 1,228\n",
      "_________________________________________________________________\n",
      "17:34:21 : None\n",
      "17:34:21 : running model.fit\n",
      "Train on 7254 samples, validate on 382 samples\n",
      "Epoch 1/20\n",
      "7254/7254 [==============================] - 40s 6ms/sample - loss: 3.3698 - acc: 0.3037 - val_loss: 2.7840 - val_acc: 0.4895\n",
      "Epoch 2/20\n",
      "7254/7254 [==============================] - 33s 4ms/sample - loss: 2.1850 - acc: 0.5455 - val_loss: 2.3551 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "7254/7254 [==============================] - 33s 4ms/sample - loss: 1.9065 - acc: 0.5793 - val_loss: 2.0801 - val_acc: 0.5236\n",
      "Epoch 4/20\n",
      "7254/7254 [==============================] - 38s 5ms/sample - loss: 1.7364 - acc: 0.5954 - val_loss: 1.9272 - val_acc: 0.5550\n",
      "Epoch 5/20\n",
      "7254/7254 [==============================] - 35s 5ms/sample - loss: 1.5965 - acc: 0.6146 - val_loss: 1.7743 - val_acc: 0.5681\n",
      "Epoch 6/20\n",
      "7254/7254 [==============================] - 34s 5ms/sample - loss: 1.4984 - acc: 0.6318 - val_loss: 1.6948 - val_acc: 0.5890\n",
      "Epoch 7/20\n",
      "7254/7254 [==============================] - 33s 5ms/sample - loss: 1.4082 - acc: 0.6431 - val_loss: 1.6487 - val_acc: 0.6021\n",
      "Epoch 8/20\n",
      "7254/7254 [==============================] - 45s 6ms/sample - loss: 1.3236 - acc: 0.6573 - val_loss: 1.6147 - val_acc: 0.6152\n",
      "Epoch 9/20\n",
      "7254/7254 [==============================] - 38s 5ms/sample - loss: 1.2526 - acc: 0.6694 - val_loss: 1.5764 - val_acc: 0.6204\n",
      "Epoch 10/20\n",
      "7254/7254 [==============================] - 45s 6ms/sample - loss: 1.2030 - acc: 0.6842 - val_loss: 1.5818 - val_acc: 0.6204\n",
      "Epoch 11/20\n",
      "7254/7254 [==============================] - 46s 6ms/sample - loss: 1.1514 - acc: 0.6890 - val_loss: 1.5664 - val_acc: 0.6178\n",
      "Epoch 12/20\n",
      "7254/7254 [==============================] - 42s 6ms/sample - loss: 1.0869 - acc: 0.7116 - val_loss: 1.5662 - val_acc: 0.6283\n",
      "Epoch 13/20\n",
      "7254/7254 [==============================] - 37s 5ms/sample - loss: 1.0423 - acc: 0.7185 - val_loss: 1.5641 - val_acc: 0.6361\n",
      "Epoch 14/20\n",
      "7254/7254 [==============================] - 38s 5ms/sample - loss: 1.0043 - acc: 0.7225 - val_loss: 1.5600 - val_acc: 0.6309\n",
      "Epoch 15/20\n",
      "7254/7254 [==============================] - 36s 5ms/sample - loss: 0.9612 - acc: 0.7308 - val_loss: 1.5562 - val_acc: 0.6414\n",
      "Epoch 16/20\n",
      "7254/7254 [==============================] - 40s 5ms/sample - loss: 0.9240 - acc: 0.7465 - val_loss: 1.5822 - val_acc: 0.6335\n",
      "Epoch 17/20\n",
      "7254/7254 [==============================] - 38s 5ms/sample - loss: 0.8953 - acc: 0.7497 - val_loss: 1.5922 - val_acc: 0.6283\n",
      "Epoch 18/20\n",
      "7254/7254 [==============================] - 48s 7ms/sample - loss: 0.8587 - acc: 0.7588 - val_loss: 1.5846 - val_acc: 0.6283\n",
      "Epoch 19/20\n",
      "7254/7254 [==============================] - 39s 5ms/sample - loss: 0.8307 - acc: 0.7604 - val_loss: 1.6070 - val_acc: 0.6204\n",
      "Epoch 20/20\n",
      "7254/7254 [==============================] - 40s 5ms/sample - loss: 0.8213 - acc: 0.7673 - val_loss: 1.5939 - val_acc: 0.6387\n",
      "17:47:22 : completed iteration =15\n",
      "17:47:22 :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84       397\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.55      0.43      0.48        14\n",
      "           3       1.00      0.67      0.80         3\n",
      "           4       0.62      0.62      0.62        26\n",
      "           5       0.47      0.47      0.47        15\n",
      "           6       0.62      0.42      0.50        12\n",
      "           7       0.00      0.00      0.00         4\n",
      "           8       0.25      0.11      0.15         9\n",
      "           9       1.00      0.75      0.86         8\n",
      "          10       0.22      0.22      0.22         9\n",
      "          11       0.25      0.14      0.18        22\n",
      "          12       0.61      0.46      0.52        24\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       1.00      0.33      0.50         3\n",
      "          15       1.00      0.33      0.50         3\n",
      "          16       0.67      1.00      0.80         2\n",
      "          17       0.82      0.93      0.87        29\n",
      "          18       0.45      0.42      0.43        12\n",
      "          19       0.33      0.17      0.22         6\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       1.00      0.25      0.40         4\n",
      "          22       0.64      0.70      0.67        10\n",
      "          23       0.44      0.40      0.42        20\n",
      "          24       0.00      0.00      0.00         4\n",
      "          25       1.00      0.29      0.44         7\n",
      "          27       0.50      0.55      0.52        11\n",
      "          28       0.20      0.17      0.18         6\n",
      "          29       0.00      0.00      0.00         1\n",
      "          30       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00         2\n",
      "          33       0.50      0.30      0.37        10\n",
      "          34       0.50      0.20      0.29         5\n",
      "          35       0.50      0.25      0.33         4\n",
      "          36       0.00      0.00      0.00         4\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.00      0.00      0.00         3\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         3\n",
      "          42       1.00      1.00      1.00         2\n",
      "          43       0.00      0.00      0.00         1\n",
      "          44       0.50      0.38      0.43        13\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         1\n",
      "          47       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         1\n",
      "          50       0.00      0.00      0.00         1\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.83      0.28      0.42        18\n",
      "          56       0.00      0.00      0.00         2\n",
      "          57       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         1\n",
      "          63       0.71      0.71      0.71         7\n",
      "          66       0.00      0.00      0.00         1\n",
      "          67       0.58      0.83      0.68        66\n",
      "          68       0.43      0.36      0.39        25\n",
      "\n",
      "    accuracy                           0.68       849\n",
      "   macro avg       0.36      0.27      0.29       849\n",
      "weighted avg       0.63      0.68      0.64       849\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:22 : total count for GRP_74=36\n",
      "17:47:22 : fitting all classic models with column_name= short_description_text\n",
      "17:47:22 : working on grp_74 with <30 rows, total groups=36 short_description_text LogisticRegression Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:23 : completed iteration =16\n",
      "17:47:23 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.71      0.96      0.81       397\n",
      "       GRP_1       1.00      0.33      0.50         3\n",
      "      GRP_10       0.70      0.50      0.58        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.62      0.50      0.55        26\n",
      "      GRP_13       0.56      0.36      0.43        14\n",
      "      GRP_14       0.71      0.42      0.53        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.80      0.50      0.62         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.33      0.11      0.17         9\n",
      "      GRP_19       0.62      0.24      0.34        21\n",
      "       GRP_2       0.43      0.25      0.32        24\n",
      "      GRP_20       1.00      0.25      0.40         4\n",
      "      GRP_22       1.00      0.33      0.50         3\n",
      "      GRP_24       0.83      0.86      0.85        29\n",
      "      GRP_25       0.50      0.17      0.25        12\n",
      "      GRP_26       1.00      0.33      0.50         6\n",
      "      GRP_28       1.00      0.25      0.40         4\n",
      "      GRP_29       0.75      0.30      0.43        10\n",
      "       GRP_3       0.62      0.25      0.36        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       1.00      0.29      0.44         7\n",
      "      GRP_33       0.67      0.36      0.47        11\n",
      "      GRP_34       1.00      0.17      0.29         6\n",
      "       GRP_4       0.50      0.60      0.55        10\n",
      "      GRP_40       1.00      0.50      0.67         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       1.00      0.38      0.56        13\n",
      "       GRP_6       0.55      0.33      0.41        18\n",
      "       GRP_7       1.00      0.43      0.60         7\n",
      "      GRP_74       0.12      0.08      0.10        36\n",
      "       GRP_8       0.57      0.86      0.69        66\n",
      "       GRP_9       0.50      0.08      0.14        25\n",
      "\n",
      "    accuracy                           0.67       849\n",
      "   macro avg       0.61      0.33      0.40       849\n",
      "weighted avg       0.64      0.67      0.62       849\n",
      "\n",
      "17:47:23 : working on grp_74 with <30 rows, total groups=36 short_description_text LogisticRegression TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:24 : completed iteration =17\n",
      "17:47:24 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.61      0.98      0.75       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       1.00      0.43      0.60        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.42      0.31      0.36        26\n",
      "      GRP_13       0.75      0.21      0.33        14\n",
      "      GRP_14       1.00      0.25      0.40        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.50      0.12      0.20         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.50      0.10      0.16        21\n",
      "       GRP_2       0.29      0.08      0.13        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.87      0.69      0.77        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       1.00      0.10      0.18        10\n",
      "       GRP_3       1.00      0.05      0.10        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       1.00      0.18      0.31        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       1.00      0.23      0.38        13\n",
      "       GRP_6       0.70      0.39      0.50        18\n",
      "       GRP_7       1.00      0.43      0.60         7\n",
      "      GRP_74       0.12      0.03      0.05        36\n",
      "       GRP_8       0.55      0.86      0.67        66\n",
      "       GRP_9       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.61       849\n",
      "   macro avg       0.39      0.18      0.21       849\n",
      "weighted avg       0.57      0.61      0.51       849\n",
      "\n",
      "17:47:24 : working on grp_74 with <30 rows, total groups=36 short_description_text DecisionTreeClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:25 : completed iteration =18\n",
      "17:47:25 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.68      0.84      0.75       397\n",
      "       GRP_1       1.00      0.33      0.50         3\n",
      "      GRP_10       0.50      0.50      0.50        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.40      0.46      0.43        26\n",
      "      GRP_13       0.50      0.36      0.42        14\n",
      "      GRP_14       0.50      0.33      0.40        12\n",
      "      GRP_15       1.00      0.25      0.40         4\n",
      "      GRP_16       0.67      0.50      0.57         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.40      0.22      0.29         9\n",
      "      GRP_19       0.13      0.10      0.11        21\n",
      "       GRP_2       0.31      0.21      0.25        24\n",
      "      GRP_20       0.50      0.25      0.33         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.80      0.69      0.74        29\n",
      "      GRP_25       0.30      0.25      0.27        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       1.00      0.30      0.46        10\n",
      "       GRP_3       0.23      0.15      0.18        20\n",
      "      GRP_30       0.33      0.25      0.29         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.08      0.09      0.09        11\n",
      "      GRP_34       1.00      0.17      0.29         6\n",
      "       GRP_4       0.50      0.50      0.50        10\n",
      "      GRP_40       0.17      0.25      0.20         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       1.00      0.46      0.63        13\n",
      "       GRP_6       0.60      0.33      0.43        18\n",
      "       GRP_7       0.80      0.57      0.67         7\n",
      "      GRP_74       0.19      0.11      0.14        36\n",
      "       GRP_8       0.60      0.79      0.68        66\n",
      "       GRP_9       0.21      0.12      0.15        25\n",
      "\n",
      "    accuracy                           0.59       849\n",
      "   macro avg       0.43      0.29      0.32       849\n",
      "weighted avg       0.55      0.59      0.55       849\n",
      "\n",
      "17:47:25 : working on grp_74 with <30 rows, total groups=36 short_description_text DecisionTreeClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:26 : completed iteration =19\n",
      "17:47:26 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.70      0.85      0.77       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.43      0.43      0.43        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.25      0.19      0.22        26\n",
      "      GRP_13       0.38      0.36      0.37        14\n",
      "      GRP_14       0.50      0.33      0.40        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.60      0.38      0.46         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.33      0.33      0.33         9\n",
      "      GRP_19       0.12      0.10      0.11        21\n",
      "       GRP_2       0.21      0.17      0.19        24\n",
      "      GRP_20       0.50      0.25      0.33         4\n",
      "      GRP_22       0.25      0.33      0.29         3\n",
      "      GRP_24       0.86      0.83      0.84        29\n",
      "      GRP_25       0.38      0.42      0.40        12\n",
      "      GRP_26       0.17      0.17      0.17         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       1.00      0.20      0.33        10\n",
      "       GRP_3       0.33      0.25      0.29        20\n",
      "      GRP_30       0.33      0.25      0.29         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.38      0.27      0.32        11\n",
      "      GRP_34       0.25      0.17      0.20         6\n",
      "       GRP_4       0.44      0.70      0.54        10\n",
      "      GRP_40       0.40      0.50      0.44         4\n",
      "      GRP_41       0.50      0.25      0.33         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       1.00      0.46      0.63        13\n",
      "       GRP_6       0.67      0.33      0.44        18\n",
      "       GRP_7       1.00      0.57      0.73         7\n",
      "      GRP_74       0.19      0.11      0.14        36\n",
      "       GRP_8       0.57      0.76      0.65        66\n",
      "       GRP_9       0.25      0.16      0.20        25\n",
      "\n",
      "    accuracy                           0.59       849\n",
      "   macro avg       0.39      0.31      0.33       849\n",
      "weighted avg       0.56      0.59      0.56       849\n",
      "\n",
      "17:47:26 : working on grp_74 with <30 rows, total groups=36 short_description_text RandomForestClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:27 : completed iteration =20\n",
      "17:47:27 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.68      0.91      0.78       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.78      0.50      0.61        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.42      0.50      0.46        26\n",
      "      GRP_13       0.33      0.21      0.26        14\n",
      "      GRP_14       0.67      0.33      0.44        12\n",
      "      GRP_15       0.67      0.50      0.57         4\n",
      "      GRP_16       0.40      0.25      0.31         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.50      0.33      0.40         9\n",
      "      GRP_19       0.27      0.14      0.19        21\n",
      "       GRP_2       0.21      0.12      0.16        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.87      0.69      0.77        29\n",
      "      GRP_25       0.38      0.42      0.40        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.57      0.40      0.47        10\n",
      "       GRP_3       0.44      0.20      0.28        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.22      0.18      0.20        11\n",
      "      GRP_34       0.50      0.17      0.25         6\n",
      "       GRP_4       0.67      0.40      0.50        10\n",
      "      GRP_40       0.50      0.25      0.33         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       1.00      0.46      0.63        13\n",
      "       GRP_6       0.54      0.39      0.45        18\n",
      "       GRP_7       1.00      0.14      0.25         7\n",
      "      GRP_74       0.10      0.03      0.04        36\n",
      "       GRP_8       0.63      0.85      0.72        66\n",
      "       GRP_9       0.40      0.16      0.23        25\n",
      "\n",
      "    accuracy                           0.62       849\n",
      "   macro avg       0.38      0.27      0.30       849\n",
      "weighted avg       0.56      0.62      0.57       849\n",
      "\n",
      "17:47:27 : working on grp_74 with <30 rows, total groups=36 short_description_text RandomForestClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:29 : completed iteration =21\n",
      "17:47:29 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.67      0.94      0.78       397\n",
      "       GRP_1       1.00      0.33      0.50         3\n",
      "      GRP_10       0.64      0.50      0.56        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.41      0.42      0.42        26\n",
      "      GRP_13       0.50      0.43      0.46        14\n",
      "      GRP_14       1.00      0.42      0.59        12\n",
      "      GRP_15       1.00      0.25      0.40         4\n",
      "      GRP_16       0.50      0.12      0.20         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.56      0.24      0.33        21\n",
      "       GRP_2       0.44      0.17      0.24        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.92      0.76      0.83        29\n",
      "      GRP_25       0.67      0.17      0.27        12\n",
      "      GRP_26       0.50      0.17      0.25         6\n",
      "      GRP_28       1.00      0.25      0.40         4\n",
      "      GRP_29       0.80      0.40      0.53        10\n",
      "       GRP_3       0.43      0.15      0.22        20\n",
      "      GRP_30       0.25      0.25      0.25         4\n",
      "      GRP_31       0.33      0.14      0.20         7\n",
      "      GRP_33       0.25      0.09      0.13        11\n",
      "      GRP_34       0.33      0.17      0.22         6\n",
      "       GRP_4       0.67      0.40      0.50        10\n",
      "      GRP_40       1.00      0.25      0.40         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       1.00      0.46      0.63        13\n",
      "       GRP_6       0.55      0.33      0.41        18\n",
      "       GRP_7       1.00      0.29      0.44         7\n",
      "      GRP_74       0.27      0.08      0.13        36\n",
      "       GRP_8       0.60      0.83      0.70        66\n",
      "       GRP_9       0.38      0.20      0.26        25\n",
      "\n",
      "    accuracy                           0.64       849\n",
      "   macro avg       0.52      0.28      0.34       849\n",
      "weighted avg       0.60      0.64      0.58       849\n",
      "\n",
      "17:47:29 : working on grp_74 with <30 rows, total groups=36 short_description_text AdaBoostClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:30 : completed iteration =22\n",
      "17:47:30 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.54      0.99      0.70       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.67      0.25      0.36         8\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        21\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       0.50      0.33      0.40         3\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "       GRP_7       0.67      0.57      0.62         7\n",
      "      GRP_74       0.00      0.00      0.00        36\n",
      "       GRP_8       0.87      0.30      0.45        66\n",
      "       GRP_9       0.20      0.68      0.30        25\n",
      "\n",
      "    accuracy                           0.51       849\n",
      "   macro avg       0.10      0.09      0.08       849\n",
      "weighted avg       0.34      0.51      0.38       849\n",
      "\n",
      "17:47:30 : working on grp_74 with <30 rows, total groups=36 short_description_text AdaBoostClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:31 : completed iteration =23\n",
      "17:47:31 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.55      0.99      0.71       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.67      0.25      0.36         8\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        21\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       1.00      0.33      0.50         3\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "       GRP_7       0.67      0.57      0.62         7\n",
      "      GRP_74       0.00      0.00      0.00        36\n",
      "       GRP_8       0.65      0.36      0.47        66\n",
      "       GRP_9       0.20      0.68      0.30        25\n",
      "\n",
      "    accuracy                           0.52       849\n",
      "   macro avg       0.10      0.09      0.08       849\n",
      "weighted avg       0.33      0.52      0.39       849\n",
      "\n",
      "17:47:31 : working on grp_74 with <30 rows, total groups=36 short_description_text BaggingClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:36 : completed iteration =24\n",
      "17:47:36 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.68      0.89      0.77       397\n",
      "       GRP_1       1.00      0.33      0.50         3\n",
      "      GRP_10       0.50      0.57      0.53        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.44      0.42      0.43        26\n",
      "      GRP_13       0.67      0.43      0.52        14\n",
      "      GRP_14       1.00      0.42      0.59        12\n",
      "      GRP_15       0.50      0.25      0.33         4\n",
      "      GRP_16       1.00      0.38      0.55         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       1.00      0.22      0.36         9\n",
      "      GRP_19       0.23      0.14      0.18        21\n",
      "       GRP_2       0.38      0.33      0.36        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.81      0.59      0.68        29\n",
      "      GRP_25       0.57      0.33      0.42        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.40      0.50      0.44         4\n",
      "      GRP_29       0.50      0.40      0.44        10\n",
      "       GRP_3       0.55      0.30      0.39        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.33      0.14      0.20         7\n",
      "      GRP_33       0.20      0.18      0.19        11\n",
      "      GRP_34       0.50      0.17      0.25         6\n",
      "       GRP_4       0.30      0.30      0.30        10\n",
      "      GRP_40       0.00      0.00      0.00         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.25      0.33      0.29         3\n",
      "       GRP_5       1.00      0.46      0.63        13\n",
      "       GRP_6       0.58      0.39      0.47        18\n",
      "       GRP_7       1.00      0.57      0.73         7\n",
      "      GRP_74       0.31      0.14      0.19        36\n",
      "       GRP_8       0.60      0.82      0.69        66\n",
      "       GRP_9       0.38      0.20      0.26        25\n",
      "\n",
      "    accuracy                           0.62       849\n",
      "   macro avg       0.46      0.31      0.35       849\n",
      "weighted avg       0.59      0.62      0.59       849\n",
      "\n",
      "17:47:36 : working on grp_74 with <30 rows, total groups=36 short_description_text BaggingClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:42 : completed iteration =25\n",
      "17:47:42 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.68      0.93      0.79       397\n",
      "       GRP_1       1.00      0.33      0.50         3\n",
      "      GRP_10       0.78      0.50      0.61        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.44      0.46      0.45        26\n",
      "      GRP_13       0.50      0.43      0.46        14\n",
      "      GRP_14       0.62      0.42      0.50        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       1.00      0.25      0.40         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.43      0.33      0.38         9\n",
      "      GRP_19       0.40      0.19      0.26        21\n",
      "       GRP_2       0.38      0.12      0.19        24\n",
      "      GRP_20       0.33      0.25      0.29         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.85      0.79      0.82        29\n",
      "      GRP_25       0.50      0.08      0.14        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       1.00      0.50      0.67         4\n",
      "      GRP_29       0.57      0.40      0.47        10\n",
      "       GRP_3       0.33      0.20      0.25        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.33      0.18      0.24        11\n",
      "      GRP_34       0.50      0.17      0.25         6\n",
      "       GRP_4       0.43      0.30      0.35        10\n",
      "      GRP_40       1.00      0.50      0.67         4\n",
      "      GRP_41       0.67      0.50      0.57         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       0.80      0.31      0.44        13\n",
      "       GRP_6       0.67      0.33      0.44        18\n",
      "       GRP_7       1.00      0.57      0.73         7\n",
      "      GRP_74       0.40      0.11      0.17        36\n",
      "       GRP_8       0.58      0.79      0.67        66\n",
      "       GRP_9       0.46      0.24      0.32        25\n",
      "\n",
      "    accuracy                           0.64       849\n",
      "   macro avg       0.49      0.31      0.36       849\n",
      "weighted avg       0.60      0.64      0.59       849\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:43 : working on grp_74 with <30 rows, total groups=36 short_description_text GaussianNB Bow\n",
      "17:47:47 : completed iteration =26\n",
      "17:47:47 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.61      0.29      0.40       397\n",
      "       GRP_1       0.07      0.33      0.12         3\n",
      "      GRP_10       0.67      0.57      0.62        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.62      0.38      0.48        26\n",
      "      GRP_13       0.50      0.50      0.50        14\n",
      "      GRP_14       0.12      0.25      0.16        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.21      0.50      0.30         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.38      0.33      0.35         9\n",
      "      GRP_19       0.14      0.24      0.17        21\n",
      "       GRP_2       0.13      0.25      0.17        24\n",
      "      GRP_20       0.10      0.25      0.14         4\n",
      "      GRP_22       0.10      0.67      0.17         3\n",
      "      GRP_24       0.86      0.86      0.86        29\n",
      "      GRP_25       0.50      0.17      0.25        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.09      0.25      0.13         4\n",
      "      GRP_29       0.29      0.20      0.24        10\n",
      "       GRP_3       0.24      0.25      0.24        20\n",
      "      GRP_30       0.43      0.75      0.55         4\n",
      "      GRP_31       0.10      0.43      0.16         7\n",
      "      GRP_33       0.08      0.09      0.08        11\n",
      "      GRP_34       0.09      0.50      0.16         6\n",
      "       GRP_4       0.27      0.70      0.39        10\n",
      "      GRP_40       0.00      0.00      0.00         4\n",
      "      GRP_41       0.11      0.25      0.15         4\n",
      "      GRP_42       0.36      1.00      0.53         4\n",
      "      GRP_45       0.20      0.33      0.25         3\n",
      "       GRP_5       0.15      0.77      0.25        13\n",
      "       GRP_6       0.67      0.33      0.44        18\n",
      "       GRP_7       0.38      0.43      0.40         7\n",
      "      GRP_74       0.22      0.19      0.21        36\n",
      "       GRP_8       0.81      0.39      0.53        66\n",
      "       GRP_9       0.14      0.12      0.13        25\n",
      "\n",
      "    accuracy                           0.34       849\n",
      "   macro avg       0.29      0.38      0.29       849\n",
      "weighted avg       0.50      0.34      0.37       849\n",
      "\n",
      "17:47:47 : working on grp_74 with <30 rows, total groups=36 short_description_text GaussianNB TFIDF\n",
      "17:47:51 : completed iteration =27\n",
      "17:47:51 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.60      0.29      0.39       397\n",
      "       GRP_1       0.07      0.33      0.12         3\n",
      "      GRP_10       0.67      0.57      0.62        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.59      0.38      0.47        26\n",
      "      GRP_13       0.46      0.43      0.44        14\n",
      "      GRP_14       0.13      0.25      0.17        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.22      0.50      0.31         8\n",
      "      GRP_17       0.89      1.00      0.94         8\n",
      "      GRP_18       0.25      0.22      0.24         9\n",
      "      GRP_19       0.14      0.24      0.17        21\n",
      "       GRP_2       0.12      0.25      0.16        24\n",
      "      GRP_20       0.10      0.25      0.14         4\n",
      "      GRP_22       0.10      0.67      0.17         3\n",
      "      GRP_24       0.81      0.86      0.83        29\n",
      "      GRP_25       0.25      0.17      0.20        12\n",
      "      GRP_26       0.07      0.17      0.10         6\n",
      "      GRP_28       0.08      0.25      0.12         4\n",
      "      GRP_29       0.40      0.20      0.27        10\n",
      "       GRP_3       0.23      0.25      0.24        20\n",
      "      GRP_30       0.43      0.75      0.55         4\n",
      "      GRP_31       0.10      0.43      0.17         7\n",
      "      GRP_33       0.09      0.09      0.09        11\n",
      "      GRP_34       0.12      0.67      0.21         6\n",
      "       GRP_4       0.25      0.60      0.35        10\n",
      "      GRP_40       0.07      0.25      0.11         4\n",
      "      GRP_41       0.11      0.25      0.15         4\n",
      "      GRP_42       0.38      0.75      0.50         4\n",
      "      GRP_45       0.17      0.33      0.22         3\n",
      "       GRP_5       0.15      0.77      0.26        13\n",
      "       GRP_6       0.67      0.33      0.44        18\n",
      "       GRP_7       0.33      0.43      0.38         7\n",
      "      GRP_74       0.22      0.19      0.21        36\n",
      "       GRP_8       0.79      0.39      0.53        66\n",
      "       GRP_9       0.15      0.12      0.13        25\n",
      "\n",
      "    accuracy                           0.34       849\n",
      "   macro avg       0.28      0.38      0.29       849\n",
      "weighted avg       0.49      0.34      0.37       849\n",
      "\n",
      "17:47:51 : working on grp_74 with <30 rows, total groups=36 short_description_text svm.svc Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:55 : completed iteration =28\n",
      "17:47:55 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.47      1.00      0.64       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.00      0.00      0.00         8\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        21\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "       GRP_7       0.00      0.00      0.00         7\n",
      "      GRP_74       0.00      0.00      0.00        36\n",
      "       GRP_8       0.00      0.00      0.00        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.47       849\n",
      "   macro avg       0.01      0.03      0.02       849\n",
      "weighted avg       0.22      0.47      0.30       849\n",
      "\n",
      "17:47:55 : working on grp_74 with <30 rows, total groups=36 short_description_text svm.svc TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:59 : completed iteration =29\n",
      "17:47:59 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.47      1.00      0.64       397\n",
      "       GRP_1       0.00      0.00      0.00         3\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_11       0.00      0.00      0.00         3\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_15       0.00      0.00      0.00         4\n",
      "      GRP_16       0.00      0.00      0.00         8\n",
      "      GRP_17       0.00      0.00      0.00         8\n",
      "      GRP_18       0.00      0.00      0.00         9\n",
      "      GRP_19       0.00      0.00      0.00        21\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_20       0.00      0.00      0.00         4\n",
      "      GRP_22       0.00      0.00      0.00         3\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "      GRP_26       0.00      0.00      0.00         6\n",
      "      GRP_28       0.00      0.00      0.00         4\n",
      "      GRP_29       0.00      0.00      0.00        10\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_30       0.00      0.00      0.00         4\n",
      "      GRP_31       0.00      0.00      0.00         7\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "      GRP_34       0.00      0.00      0.00         6\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "      GRP_40       0.00      0.00      0.00         4\n",
      "      GRP_41       0.00      0.00      0.00         4\n",
      "      GRP_42       0.00      0.00      0.00         4\n",
      "      GRP_45       0.00      0.00      0.00         3\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "       GRP_7       0.00      0.00      0.00         7\n",
      "      GRP_74       0.00      0.00      0.00        36\n",
      "       GRP_8       0.00      0.00      0.00        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.47       849\n",
      "   macro avg       0.01      0.03      0.02       849\n",
      "weighted avg       0.22      0.47      0.30       849\n",
      "\n",
      "17:47:59 : working on grp_74 with <30 rows, total groups=36 short_description_list Keras LSTM Model processed\n",
      "17:47:59 : maxlen after= 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:47:59 : My vocabulary size =  5258\n",
      "17:48:00 : Embeddings from  glove.6B.200d.txt already exists\n",
      "17:48:00 : No. of embeddings =  400000\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 200)           1051600   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 40, 200)           800       \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 40, 144)           157248    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 144)               576       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 72)                10440     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 72)                288       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 36)                2628      \n",
      "=================================================================\n",
      "Total params: 1,223,580\n",
      "Trainable params: 1,222,748\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "17:48:00 : None\n",
      "17:48:00 : running model.fit\n",
      "Train on 7254 samples, validate on 382 samples\n",
      "Epoch 1/20\n",
      "7254/7254 [==============================] - 38s 5ms/sample - loss: 3.3166 - acc: 0.2229 - val_loss: 2.6014 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "7254/7254 [==============================] - 30s 4ms/sample - loss: 2.2404 - acc: 0.5021 - val_loss: 2.0978 - val_acc: 0.4948\n",
      "Epoch 3/20\n",
      "7254/7254 [==============================] - 24s 3ms/sample - loss: 1.9418 - acc: 0.5525 - val_loss: 1.8614 - val_acc: 0.5288\n",
      "Epoch 4/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.8075 - acc: 0.5670 - val_loss: 1.6661 - val_acc: 0.5628\n",
      "Epoch 5/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.7095 - acc: 0.5797 - val_loss: 1.5241 - val_acc: 0.6099\n",
      "Epoch 6/20\n",
      "7254/7254 [==============================] - 22s 3ms/sample - loss: 1.6168 - acc: 0.5950 - val_loss: 1.4347 - val_acc: 0.6309\n",
      "Epoch 7/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.5504 - acc: 0.5998 - val_loss: 1.3826 - val_acc: 0.6571\n",
      "Epoch 8/20\n",
      "7254/7254 [==============================] - 25s 3ms/sample - loss: 1.4654 - acc: 0.6181 - val_loss: 1.3754 - val_acc: 0.6571\n",
      "Epoch 9/20\n",
      "7254/7254 [==============================] - 24s 3ms/sample - loss: 1.4278 - acc: 0.6263 - val_loss: 1.3077 - val_acc: 0.6754\n",
      "Epoch 10/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.3568 - acc: 0.6347 - val_loss: 1.3045 - val_acc: 0.6702\n",
      "Epoch 11/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.3178 - acc: 0.6419 - val_loss: 1.2817 - val_acc: 0.6754\n",
      "Epoch 12/20\n",
      "7254/7254 [==============================] - 22s 3ms/sample - loss: 1.2679 - acc: 0.6562 - val_loss: 1.2648 - val_acc: 0.6649\n",
      "Epoch 13/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.2301 - acc: 0.6576 - val_loss: 1.2673 - val_acc: 0.6675\n",
      "Epoch 14/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.1760 - acc: 0.6737 - val_loss: 1.2422 - val_acc: 0.6806\n",
      "Epoch 15/20\n",
      "7254/7254 [==============================] - 22s 3ms/sample - loss: 1.1544 - acc: 0.6744 - val_loss: 1.2108 - val_acc: 0.6728\n",
      "Epoch 16/20\n",
      "7254/7254 [==============================] - 24s 3ms/sample - loss: 1.1068 - acc: 0.6822 - val_loss: 1.2487 - val_acc: 0.6466\n",
      "Epoch 17/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.0753 - acc: 0.6951 - val_loss: 1.2461 - val_acc: 0.6466\n",
      "Epoch 18/20\n",
      "7254/7254 [==============================] - 23s 3ms/sample - loss: 1.0513 - acc: 0.7004 - val_loss: 1.2385 - val_acc: 0.6702\n",
      "Epoch 19/20\n",
      "7254/7254 [==============================] - 22s 3ms/sample - loss: 1.0204 - acc: 0.7069 - val_loss: 1.2126 - val_acc: 0.6597\n",
      "Epoch 20/20\n",
      "7254/7254 [==============================] - 24s 3ms/sample - loss: 1.0059 - acc: 0.7072 - val_loss: 1.2253 - val_acc: 0.6545\n",
      "17:56:08 : completed iteration =30\n",
      "17:56:08 :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82       397\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.56      0.36      0.43        14\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.81      0.50      0.62        26\n",
      "           5       0.35      0.50      0.41        14\n",
      "           6       0.50      0.08      0.14        12\n",
      "           7       1.00      0.50      0.67         4\n",
      "           8       0.75      0.38      0.50         8\n",
      "           9       1.00      1.00      1.00         8\n",
      "          10       1.00      0.11      0.20         9\n",
      "          11       0.22      0.10      0.13        21\n",
      "          12       0.50      0.21      0.29        24\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.79      0.90      0.84        29\n",
      "          16       0.50      0.17      0.25        12\n",
      "          17       0.00      0.00      0.00         6\n",
      "          18       0.00      0.00      0.00         4\n",
      "          19       0.62      0.50      0.56        10\n",
      "          20       0.54      0.35      0.42        20\n",
      "          21       1.00      0.25      0.40         4\n",
      "          22       0.50      0.14      0.22         7\n",
      "          23       0.45      0.45      0.45        11\n",
      "          24       0.50      0.17      0.25         6\n",
      "          25       0.50      0.20      0.29        10\n",
      "          26       0.00      0.00      0.00         4\n",
      "          27       0.00      0.00      0.00         4\n",
      "          28       0.00      0.00      0.00         4\n",
      "          29       0.00      0.00      0.00         3\n",
      "          30       0.55      0.46      0.50        13\n",
      "          31       0.60      0.17      0.26        18\n",
      "          32       0.67      0.86      0.75         7\n",
      "          33       0.39      0.25      0.31        36\n",
      "          34       0.75      0.59      0.66        66\n",
      "          35       0.32      0.76      0.45        25\n",
      "\n",
      "    accuracy                           0.66       849\n",
      "   macro avg       0.45      0.30      0.33       849\n",
      "weighted avg       0.62      0.66      0.61       849\n",
      "\n",
      "17:56:08 : total count for GRP_74=17\n",
      "17:56:08 : fitting all classic models with column_name= short_description_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:08 : working on grp_74 with <100 rows, total groups=17 short_description_text LogisticRegression Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:08 : completed iteration =31\n",
      "17:56:08 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.73      0.92      0.81       397\n",
      "      GRP_10       1.00      0.29      0.44        14\n",
      "      GRP_12       0.41      0.27      0.33        26\n",
      "      GRP_13       0.29      0.14      0.19        14\n",
      "      GRP_14       0.67      0.17      0.27        12\n",
      "      GRP_19       0.33      0.05      0.08        22\n",
      "       GRP_2       0.64      0.38      0.47        24\n",
      "      GRP_24       0.90      0.90      0.90        29\n",
      "      GRP_25       0.50      0.17      0.25        12\n",
      "       GRP_3       0.45      0.25      0.32        20\n",
      "      GRP_33       0.83      0.45      0.59        11\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "       GRP_5       1.00      0.38      0.56        13\n",
      "       GRP_6       0.57      0.22      0.32        18\n",
      "      GRP_74       0.52      0.49      0.50       136\n",
      "       GRP_8       0.56      0.89      0.69        66\n",
      "       GRP_9       0.50      0.08      0.14        25\n",
      "\n",
      "    accuracy                           0.66       849\n",
      "   macro avg       0.58      0.36      0.40       849\n",
      "weighted avg       0.64      0.66      0.62       849\n",
      "\n",
      "17:56:08 : working on grp_74 with <100 rows, total groups=17 short_description_text LogisticRegression TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:09 : completed iteration =32\n",
      "17:56:09 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.69      0.94      0.80       397\n",
      "      GRP_10       1.00      0.21      0.35        14\n",
      "      GRP_12       0.40      0.23      0.29        26\n",
      "      GRP_13       0.50      0.07      0.12        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.50      0.17      0.25        24\n",
      "      GRP_24       0.92      0.76      0.83        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "       GRP_3       1.00      0.05      0.10        20\n",
      "      GRP_33       1.00      0.27      0.43        11\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "       GRP_5       0.83      0.38      0.53        13\n",
      "       GRP_6       0.67      0.22      0.33        18\n",
      "      GRP_74       0.47      0.47      0.47       136\n",
      "       GRP_8       0.55      0.86      0.67        66\n",
      "       GRP_9       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.64       849\n",
      "   macro avg       0.56      0.28      0.31       849\n",
      "weighted avg       0.62      0.64      0.58       849\n",
      "\n",
      "17:56:09 : working on grp_74 with <100 rows, total groups=17 short_description_text DecisionTreeClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:09 : completed iteration =33\n",
      "17:56:09 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.74      0.83      0.78       397\n",
      "      GRP_10       0.62      0.36      0.45        14\n",
      "      GRP_12       0.37      0.38      0.38        26\n",
      "      GRP_13       0.14      0.07      0.10        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.25      0.14      0.18        22\n",
      "       GRP_2       0.40      0.42      0.41        24\n",
      "      GRP_24       0.75      0.83      0.79        29\n",
      "      GRP_25       0.50      0.33      0.40        12\n",
      "       GRP_3       0.33      0.20      0.25        20\n",
      "      GRP_33       0.20      0.09      0.13        11\n",
      "       GRP_4       0.50      0.40      0.44        10\n",
      "       GRP_5       0.73      0.62      0.67        13\n",
      "       GRP_6       0.31      0.28      0.29        18\n",
      "      GRP_74       0.44      0.40      0.42       136\n",
      "       GRP_8       0.60      0.80      0.68        66\n",
      "       GRP_9       0.33      0.24      0.28        25\n",
      "\n",
      "    accuracy                           0.61       849\n",
      "   macro avg       0.42      0.38      0.39       849\n",
      "weighted avg       0.58      0.61      0.59       849\n",
      "\n",
      "17:56:09 : working on grp_74 with <100 rows, total groups=17 short_description_text DecisionTreeClassifier TFIDF\n",
      "17:56:10 : completed iteration =34\n",
      "17:56:10 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.77      0.81      0.79       397\n",
      "      GRP_10       0.75      0.43      0.55        14\n",
      "      GRP_12       0.36      0.31      0.33        26\n",
      "      GRP_13       0.14      0.07      0.10        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.29      0.18      0.22        22\n",
      "       GRP_2       0.52      0.46      0.49        24\n",
      "      GRP_24       0.79      0.76      0.77        29\n",
      "      GRP_25       0.29      0.17      0.21        12\n",
      "       GRP_3       0.30      0.30      0.30        20\n",
      "      GRP_33       0.29      0.18      0.22        11\n",
      "       GRP_4       0.56      0.50      0.53        10\n",
      "       GRP_5       0.73      0.62      0.67        13\n",
      "       GRP_6       0.44      0.22      0.30        18\n",
      "      GRP_74       0.39      0.46      0.42       136\n",
      "       GRP_8       0.59      0.80      0.68        66\n",
      "       GRP_9       0.44      0.32      0.37        25\n",
      "\n",
      "    accuracy                           0.62       849\n",
      "   macro avg       0.45      0.39      0.41       849\n",
      "weighted avg       0.60      0.62      0.60       849\n",
      "\n",
      "17:56:10 : working on grp_74 with <100 rows, total groups=17 short_description_text RandomForestClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:11 : completed iteration =35\n",
      "17:56:11 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.71      0.90      0.79       397\n",
      "      GRP_10       0.71      0.36      0.48        14\n",
      "      GRP_12       0.38      0.38      0.38        26\n",
      "      GRP_13       0.25      0.07      0.11        14\n",
      "      GRP_14       0.50      0.08      0.14        12\n",
      "      GRP_19       0.30      0.14      0.19        22\n",
      "       GRP_2       0.53      0.38      0.44        24\n",
      "      GRP_24       0.82      0.79      0.81        29\n",
      "      GRP_25       0.50      0.08      0.14        12\n",
      "       GRP_3       0.33      0.15      0.21        20\n",
      "      GRP_33       0.40      0.18      0.25        11\n",
      "       GRP_4       0.50      0.10      0.17        10\n",
      "       GRP_5       0.80      0.62      0.70        13\n",
      "       GRP_6       0.32      0.33      0.32        18\n",
      "      GRP_74       0.50      0.36      0.42       136\n",
      "       GRP_8       0.59      0.82      0.69        66\n",
      "       GRP_9       0.54      0.28      0.37        25\n",
      "\n",
      "    accuracy                           0.64       849\n",
      "   macro avg       0.51      0.35      0.39       849\n",
      "weighted avg       0.60      0.64      0.60       849\n",
      "\n",
      "17:56:11 : working on grp_74 with <100 rows, total groups=17 short_description_text RandomForestClassifier TFIDF\n",
      "17:56:12 : completed iteration =36\n",
      "17:56:12 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.71      0.91      0.80       397\n",
      "      GRP_10       0.62      0.36      0.45        14\n",
      "      GRP_12       0.48      0.50      0.49        26\n",
      "      GRP_13       0.33      0.21      0.26        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.43      0.14      0.21        22\n",
      "       GRP_2       0.64      0.38      0.47        24\n",
      "      GRP_24       0.79      0.90      0.84        29\n",
      "      GRP_25       1.00      0.08      0.15        12\n",
      "       GRP_3       0.17      0.10      0.12        20\n",
      "      GRP_33       0.75      0.27      0.40        11\n",
      "       GRP_4       0.33      0.10      0.15        10\n",
      "       GRP_5       0.80      0.62      0.70        13\n",
      "       GRP_6       0.71      0.28      0.40        18\n",
      "      GRP_74       0.52      0.38      0.44       136\n",
      "       GRP_8       0.60      0.82      0.69        66\n",
      "       GRP_9       0.54      0.28      0.37        25\n",
      "\n",
      "    accuracy                           0.65       849\n",
      "   macro avg       0.55      0.37      0.41       849\n",
      "weighted avg       0.62      0.65      0.62       849\n",
      "\n",
      "17:56:12 : working on grp_74 with <100 rows, total groups=17 short_description_text AdaBoostClassifier Bow\n",
      "17:56:13 : completed iteration =37\n",
      "17:56:13 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.58      0.98      0.73       397\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_12       0.42      0.38      0.40        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_24       0.86      0.41      0.56        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "       GRP_5       0.12      0.85      0.21        13\n",
      "       GRP_6       0.43      0.17      0.24        18\n",
      "      GRP_74       0.20      0.01      0.01       136\n",
      "       GRP_8       0.79      0.33      0.47        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.53       849\n",
      "   macro avg       0.20      0.18      0.15       849\n",
      "weighted avg       0.42      0.53      0.42       849\n",
      "\n",
      "17:56:13 : working on grp_74 with <100 rows, total groups=17 short_description_text AdaBoostClassifier TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:14 : completed iteration =38\n",
      "17:56:14 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.55      0.99      0.70       397\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_12       0.25      0.04      0.07        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "       GRP_5       1.00      0.15      0.27        13\n",
      "       GRP_6       0.43      0.17      0.24        18\n",
      "      GRP_74       0.00      0.00      0.00       136\n",
      "       GRP_8       0.50      0.88      0.63        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.54       849\n",
      "   macro avg       0.16      0.13      0.11       849\n",
      "weighted avg       0.33      0.54      0.39       849\n",
      "\n",
      "17:56:14 : working on grp_74 with <100 rows, total groups=17 short_description_text BaggingClassifier Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:18 : completed iteration =39\n",
      "17:56:18 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.75      0.86      0.80       397\n",
      "      GRP_10       0.83      0.36      0.50        14\n",
      "      GRP_12       0.41      0.42      0.42        26\n",
      "      GRP_13       0.25      0.14      0.18        14\n",
      "      GRP_14       1.00      0.08      0.15        12\n",
      "      GRP_19       0.27      0.14      0.18        22\n",
      "       GRP_2       0.41      0.38      0.39        24\n",
      "      GRP_24       0.79      0.76      0.77        29\n",
      "      GRP_25       0.43      0.25      0.32        12\n",
      "       GRP_3       0.53      0.40      0.46        20\n",
      "      GRP_33       0.33      0.36      0.35        11\n",
      "       GRP_4       0.40      0.20      0.27        10\n",
      "       GRP_5       1.00      0.62      0.76        13\n",
      "       GRP_6       0.33      0.22      0.27        18\n",
      "      GRP_74       0.44      0.40      0.42       136\n",
      "       GRP_8       0.59      0.82      0.68        66\n",
      "       GRP_9       0.50      0.28      0.36        25\n",
      "\n",
      "    accuracy                           0.64       849\n",
      "   macro avg       0.54      0.39      0.43       849\n",
      "weighted avg       0.62      0.64      0.61       849\n",
      "\n",
      "17:56:18 : working on grp_74 with <100 rows, total groups=17 short_description_text BaggingClassifier TFIDF\n",
      "17:56:22 : completed iteration =40\n",
      "17:56:22 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.77      0.90      0.83       397\n",
      "      GRP_10       0.71      0.36      0.48        14\n",
      "      GRP_12       0.42      0.50      0.46        26\n",
      "      GRP_13       0.14      0.07      0.10        14\n",
      "      GRP_14       0.50      0.08      0.14        12\n",
      "      GRP_19       0.33      0.14      0.19        22\n",
      "       GRP_2       0.43      0.25      0.32        24\n",
      "      GRP_24       0.87      0.90      0.88        29\n",
      "      GRP_25       0.60      0.25      0.35        12\n",
      "       GRP_3       0.47      0.35      0.40        20\n",
      "      GRP_33       0.40      0.18      0.25        11\n",
      "       GRP_4       0.50      0.30      0.37        10\n",
      "       GRP_5       0.73      0.62      0.67        13\n",
      "       GRP_6       0.67      0.22      0.33        18\n",
      "      GRP_74       0.47      0.49      0.48       136\n",
      "       GRP_8       0.60      0.77      0.68        66\n",
      "       GRP_9       0.53      0.32      0.40        25\n",
      "\n",
      "    accuracy                           0.66       849\n",
      "   macro avg       0.54      0.39      0.43       849\n",
      "weighted avg       0.64      0.66      0.64       849\n",
      "\n",
      "17:56:22 : working on grp_74 with <100 rows, total groups=17 short_description_text GaussianNB Bow\n",
      "17:56:25 : completed iteration =41\n",
      "17:56:25 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.74      0.29      0.42       397\n",
      "      GRP_10       0.60      0.43      0.50        14\n",
      "      GRP_12       0.42      0.38      0.40        26\n",
      "      GRP_13       0.27      0.29      0.28        14\n",
      "      GRP_14       0.02      0.08      0.04        12\n",
      "      GRP_19       0.09      0.32      0.14        22\n",
      "       GRP_2       0.15      0.38      0.21        24\n",
      "      GRP_24       0.74      0.79      0.77        29\n",
      "      GRP_25       0.13      0.17      0.15        12\n",
      "       GRP_3       0.17      0.20      0.19        20\n",
      "      GRP_33       0.19      0.55      0.29        11\n",
      "       GRP_4       0.16      0.50      0.24        10\n",
      "       GRP_5       0.11      0.77      0.20        13\n",
      "       GRP_6       0.18      0.22      0.20        18\n",
      "      GRP_74       0.43      0.49      0.46       136\n",
      "       GRP_8       0.76      0.33      0.46        66\n",
      "       GRP_9       0.09      0.12      0.11        25\n",
      "\n",
      "    accuracy                           0.35       849\n",
      "   macro avg       0.31      0.37      0.30       849\n",
      "weighted avg       0.55      0.35      0.39       849\n",
      "\n",
      "17:56:25 : working on grp_74 with <100 rows, total groups=17 short_description_text GaussianNB TFIDF\n",
      "17:56:27 : completed iteration =42\n",
      "17:56:27 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.71      0.30      0.42       397\n",
      "      GRP_10       0.55      0.43      0.48        14\n",
      "      GRP_12       0.38      0.35      0.36        26\n",
      "      GRP_13       0.29      0.29      0.29        14\n",
      "      GRP_14       0.02      0.08      0.04        12\n",
      "      GRP_19       0.11      0.32      0.16        22\n",
      "       GRP_2       0.13      0.42      0.20        24\n",
      "      GRP_24       0.85      0.76      0.80        29\n",
      "      GRP_25       0.07      0.08      0.07        12\n",
      "       GRP_3       0.14      0.15      0.14        20\n",
      "      GRP_33       0.18      0.55      0.27        11\n",
      "       GRP_4       0.12      0.40      0.19        10\n",
      "       GRP_5       0.11      0.77      0.20        13\n",
      "       GRP_6       0.22      0.22      0.22        18\n",
      "      GRP_74       0.42      0.48      0.45       136\n",
      "       GRP_8       0.85      0.33      0.48        66\n",
      "       GRP_9       0.10      0.12      0.11        25\n",
      "\n",
      "    accuracy                           0.35       849\n",
      "   macro avg       0.31      0.36      0.29       849\n",
      "weighted avg       0.54      0.35      0.39       849\n",
      "\n",
      "17:56:27 : working on grp_74 with <100 rows, total groups=17 short_description_text svm.svc Bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:31 : completed iteration =43\n",
      "17:56:31 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.47      1.00      0.64       397\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "      GRP_74       0.00      0.00      0.00       136\n",
      "       GRP_8       0.00      0.00      0.00        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.47       849\n",
      "   macro avg       0.03      0.06      0.04       849\n",
      "weighted avg       0.22      0.47      0.30       849\n",
      "\n",
      "17:56:31 : working on grp_74 with <100 rows, total groups=17 short_description_text svm.svc TFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:35 : completed iteration =44\n",
      "17:56:35 :               precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.47      1.00      0.64       397\n",
      "      GRP_10       0.00      0.00      0.00        14\n",
      "      GRP_12       0.00      0.00      0.00        26\n",
      "      GRP_13       0.00      0.00      0.00        14\n",
      "      GRP_14       0.00      0.00      0.00        12\n",
      "      GRP_19       0.00      0.00      0.00        22\n",
      "       GRP_2       0.00      0.00      0.00        24\n",
      "      GRP_24       0.00      0.00      0.00        29\n",
      "      GRP_25       0.00      0.00      0.00        12\n",
      "       GRP_3       0.00      0.00      0.00        20\n",
      "      GRP_33       0.00      0.00      0.00        11\n",
      "       GRP_4       0.00      0.00      0.00        10\n",
      "       GRP_5       0.00      0.00      0.00        13\n",
      "       GRP_6       0.00      0.00      0.00        18\n",
      "      GRP_74       0.00      0.00      0.00       136\n",
      "       GRP_8       0.00      0.00      0.00        66\n",
      "       GRP_9       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.47       849\n",
      "   macro avg       0.03      0.06      0.04       849\n",
      "weighted avg       0.22      0.47      0.30       849\n",
      "\n",
      "17:56:35 : working on grp_74 with <100 rows, total groups=17 short_description_list Keras LSTM Model processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:56:35 : maxlen after= 40\n",
      "17:56:35 : My vocabulary size =  5258\n",
      "17:56:35 : Embeddings from  glove.6B.200d.txt already exists\n",
      "17:56:35 : No. of embeddings =  400000\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 200)           1051600   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 40, 200)           800       \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 40, 68)            63920     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 68)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 68)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 68)                272       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 34)                2346      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 34)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 34)                136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 17)                595       \n",
      "=================================================================\n",
      "Total params: 1,119,669\n",
      "Trainable params: 1,119,065\n",
      "Non-trainable params: 604\n",
      "_________________________________________________________________\n",
      "17:56:36 : None\n",
      "17:56:36 : running model.fit\n",
      "Train on 7254 samples, validate on 382 samples\n",
      "Epoch 1/20\n",
      "7254/7254 [==============================] - 27s 4ms/sample - loss: 2.9468 - acc: 0.1317 - val_loss: 2.3329 - val_acc: 0.5079\n",
      "Epoch 2/20\n",
      "7254/7254 [==============================] - 19s 3ms/sample - loss: 2.1213 - acc: 0.4173 - val_loss: 1.7540 - val_acc: 0.5131\n",
      "Epoch 3/20\n",
      "7254/7254 [==============================] - 20s 3ms/sample - loss: 1.7657 - acc: 0.5157 - val_loss: 1.5793 - val_acc: 0.5314\n",
      "Epoch 4/20\n",
      "7254/7254 [==============================] - 20s 3ms/sample - loss: 1.6004 - acc: 0.5551 - val_loss: 1.4820 - val_acc: 0.5576\n",
      "Epoch 5/20\n",
      "7254/7254 [==============================] - 19s 3ms/sample - loss: 1.5158 - acc: 0.5717 - val_loss: 1.4004 - val_acc: 0.5812\n",
      "Epoch 6/20\n",
      "7254/7254 [==============================] - 19s 3ms/sample - loss: 1.4434 - acc: 0.5849 - val_loss: 1.3391 - val_acc: 0.6021\n",
      "Epoch 7/20\n",
      "7254/7254 [==============================] - 20s 3ms/sample - loss: 1.3905 - acc: 0.5987 - val_loss: 1.3006 - val_acc: 0.6178\n",
      "Epoch 8/20\n",
      "7254/7254 [==============================] - 20s 3ms/sample - loss: 1.3444 - acc: 0.6103 - val_loss: 1.2727 - val_acc: 0.6283\n",
      "Epoch 9/20\n",
      "7254/7254 [==============================] - 30s 4ms/sample - loss: 1.2973 - acc: 0.6201 - val_loss: 1.2546 - val_acc: 0.6335\n",
      "Epoch 10/20\n",
      "7254/7254 [==============================] - 24s 3ms/sample - loss: 1.2639 - acc: 0.6348 - val_loss: 1.2440 - val_acc: 0.6204\n",
      "Epoch 11/20\n",
      "7254/7254 [==============================] - 18s 2ms/sample - loss: 1.2224 - acc: 0.6438 - val_loss: 1.2248 - val_acc: 0.6440\n",
      "Epoch 12/20\n",
      "7254/7254 [==============================] - 17s 2ms/sample - loss: 1.2015 - acc: 0.6377 - val_loss: 1.2061 - val_acc: 0.6466\n",
      "Epoch 13/20\n",
      "7254/7254 [==============================] - 17s 2ms/sample - loss: 1.1639 - acc: 0.6588 - val_loss: 1.1961 - val_acc: 0.6466\n",
      "Epoch 14/20\n",
      "7254/7254 [==============================] - 18s 2ms/sample - loss: 1.1353 - acc: 0.6570 - val_loss: 1.1696 - val_acc: 0.6545\n",
      "Epoch 15/20\n",
      "7254/7254 [==============================] - 18s 2ms/sample - loss: 1.1156 - acc: 0.6631 - val_loss: 1.1748 - val_acc: 0.6518\n",
      "Epoch 16/20\n",
      "7254/7254 [==============================] - 17s 2ms/sample - loss: 1.0851 - acc: 0.6758 - val_loss: 1.1839 - val_acc: 0.6414\n",
      "Epoch 17/20\n",
      "7254/7254 [==============================] - 17s 2ms/sample - loss: 1.0734 - acc: 0.6729 - val_loss: 1.1711 - val_acc: 0.6492\n",
      "Epoch 18/20\n",
      "7254/7254 [==============================] - 18s 2ms/sample - loss: 1.0351 - acc: 0.6849 - val_loss: 1.1653 - val_acc: 0.6466\n",
      "Epoch 19/20\n",
      "7254/7254 [==============================] - 18s 2ms/sample - loss: 1.0247 - acc: 0.6846 - val_loss: 1.1599 - val_acc: 0.6623\n",
      "Epoch 20/20\n",
      "7254/7254 [==============================] - 17s 2ms/sample - loss: 1.0087 - acc: 0.6937 - val_loss: 1.1619 - val_acc: 0.6492\n",
      "18:03:11 : completed iteration =45\n",
      "18:03:11 :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.94      0.82       397\n",
      "           1       1.00      0.21      0.35        14\n",
      "           2       0.46      0.50      0.48        26\n",
      "           3       0.67      0.14      0.24        14\n",
      "           4       0.00      0.00      0.00        12\n",
      "           5       0.50      0.09      0.15        22\n",
      "           6       1.00      0.04      0.08        24\n",
      "           7       0.83      0.86      0.85        29\n",
      "           8       0.00      0.00      0.00        12\n",
      "           9       0.33      0.05      0.09        20\n",
      "          10       0.50      0.27      0.35        11\n",
      "          11       0.00      0.00      0.00        10\n",
      "          12       1.00      0.38      0.56        13\n",
      "          13       0.80      0.22      0.35        18\n",
      "          14       0.49      0.48      0.49       136\n",
      "          15       0.58      0.92      0.71        66\n",
      "          16       0.71      0.20      0.31        25\n",
      "\n",
      "    accuracy                           0.66       849\n",
      "   macro avg       0.57      0.31      0.34       849\n",
      "weighted avg       0.64      0.66      0.61       849\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A1011153\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# start model building with all 74 groups , then reduce the number of groups by aggregating low sample groups into one large group\n",
    "## Perfrom 3 iteration with all models\n",
    "itr_cnt=0   \n",
    "for i in [2,30,100]:  \n",
    "    df[\"assigned_group\"]=df[\"assigned_group_org\"]\n",
    "    f=( df.assigned_group.value_counts() < i ) \n",
    "    j=f[f].index\n",
    "    df.assigned_group[df.assigned_group.isin(j)]=\"GRP_74\"\n",
    "    new_groups=df.assigned_group.nunique()\n",
    "    print_msg(\"total count for GRP_74={}\".format(new_groups))\n",
    "    msg_grp=\"grp_74 with <{} rows, total groups={}\".format(i,new_groups)\n",
    "    model_column=\" \"\n",
    "    model_name=\" \"\n",
    "    data_set=\" \"\n",
    "    sklearn_model(\"short_description_text\")\n",
    "#     sklearn_model(\"combined_description_text\")\n",
    "    keras_model(\"short_description_list\",20,1)\n",
    "#     keras_model(\"combined_description_list\",20,1)\n",
    "    log.to_excel(\"log.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>groups</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_column</th>\n",
       "      <th>data_set</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Score</th>\n",
       "      <th>Recall Score</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>kappa_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;2 rows, total groups=69</td>\n",
       "      <td>Keras LSTM Model</td>\n",
       "      <td>short_description_list</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.677267</td>\n",
       "      <td>0.525336</td>\n",
       "      <td>0.395995</td>\n",
       "      <td>0.427204</td>\n",
       "      <td>0.552353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;2 rows, total groups=69</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>short_description_text</td>\n",
       "      <td>Bow</td>\n",
       "      <td>0.676090</td>\n",
       "      <td>0.590135</td>\n",
       "      <td>0.371567</td>\n",
       "      <td>0.426145</td>\n",
       "      <td>0.531342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;2 rows, total groups=69</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>short_description_text</td>\n",
       "      <td>Bow</td>\n",
       "      <td>0.676090</td>\n",
       "      <td>0.590135</td>\n",
       "      <td>0.371567</td>\n",
       "      <td>0.426145</td>\n",
       "      <td>0.531342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;100 rows, total groups=17</td>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>short_description_text</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>0.663133</td>\n",
       "      <td>0.537706</td>\n",
       "      <td>0.393470</td>\n",
       "      <td>0.430700</td>\n",
       "      <td>0.523767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;30 rows, total groups=36</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>short_description_text</td>\n",
       "      <td>Bow</td>\n",
       "      <td>0.669022</td>\n",
       "      <td>0.732875</td>\n",
       "      <td>0.400116</td>\n",
       "      <td>0.479534</td>\n",
       "      <td>0.518314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;30 rows, total groups=36</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>short_description_text</td>\n",
       "      <td>Bow</td>\n",
       "      <td>0.669022</td>\n",
       "      <td>0.732875</td>\n",
       "      <td>0.400116</td>\n",
       "      <td>0.479534</td>\n",
       "      <td>0.518314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;100 rows, total groups=17</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>short_description_text</td>\n",
       "      <td>Bow</td>\n",
       "      <td>0.664311</td>\n",
       "      <td>0.582486</td>\n",
       "      <td>0.355186</td>\n",
       "      <td>0.403217</td>\n",
       "      <td>0.511237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;2 rows, total groups=69</td>\n",
       "      <td>Keras LSTM Model</td>\n",
       "      <td>short_description_list</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.641932</td>\n",
       "      <td>0.489569</td>\n",
       "      <td>0.404149</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.508811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;30 rows, total groups=36</td>\n",
       "      <td>Keras LSTM Model</td>\n",
       "      <td>short_description_list</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.656066</td>\n",
       "      <td>0.574363</td>\n",
       "      <td>0.389137</td>\n",
       "      <td>0.422317</td>\n",
       "      <td>0.506353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_74 with &lt;100 rows, total groups=17</td>\n",
       "      <td>Keras LSTM Model</td>\n",
       "      <td>short_description_list</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.664311</td>\n",
       "      <td>0.686335</td>\n",
       "      <td>0.380400</td>\n",
       "      <td>0.415979</td>\n",
       "      <td>0.504596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   groups          model_name  \\\n",
       "0    grp_74 with <2 rows, total groups=69    Keras LSTM Model   \n",
       "0    grp_74 with <2 rows, total groups=69  LogisticRegression   \n",
       "0    grp_74 with <2 rows, total groups=69  LogisticRegression   \n",
       "0  grp_74 with <100 rows, total groups=17   BaggingClassifier   \n",
       "0   grp_74 with <30 rows, total groups=36  LogisticRegression   \n",
       "0   grp_74 with <30 rows, total groups=36  LogisticRegression   \n",
       "0  grp_74 with <100 rows, total groups=17  LogisticRegression   \n",
       "0    grp_74 with <2 rows, total groups=69    Keras LSTM Model   \n",
       "0   grp_74 with <30 rows, total groups=36    Keras LSTM Model   \n",
       "0  grp_74 with <100 rows, total groups=17    Keras LSTM Model   \n",
       "\n",
       "             model_column   data_set  Accuracy  Precision Score  Recall Score  \\\n",
       "0  short_description_list  processed  0.677267         0.525336      0.395995   \n",
       "0  short_description_text        Bow  0.676090         0.590135      0.371567   \n",
       "0  short_description_text        Bow  0.676090         0.590135      0.371567   \n",
       "0  short_description_text      TFIDF  0.663133         0.537706      0.393470   \n",
       "0  short_description_text        Bow  0.669022         0.732875      0.400116   \n",
       "0  short_description_text        Bow  0.669022         0.732875      0.400116   \n",
       "0  short_description_text        Bow  0.664311         0.582486      0.355186   \n",
       "0  short_description_list  processed  0.641932         0.489569      0.404149   \n",
       "0  short_description_list  processed  0.656066         0.574363      0.389137   \n",
       "0  short_description_list  processed  0.664311         0.686335      0.380400   \n",
       "\n",
       "   F1-Score  kappa_score  \n",
       "0  0.427204     0.552353  \n",
       "0  0.426145     0.531342  \n",
       "0  0.426145     0.531342  \n",
       "0  0.430700     0.523767  \n",
       "0  0.479534     0.518314  \n",
       "0  0.479534     0.518314  \n",
       "0  0.403217     0.511237  \n",
       "0  0.420200     0.508811  \n",
       "0  0.422317     0.506353  \n",
       "0  0.415979     0.504596  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.sort_values(by=['kappa_score'],ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\work\\\\capstone\")\n",
    "le,tokenizer,model=model_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:58:20 : text= ['crm add-in is getting disabled from outlook ']        predicted_class= ['GRP_0']\n"
     ]
    }
   ],
   "source": [
    "model_predict(\"crm add-in is getting disabled from outlook \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model out of all iterations coming with LSTM with 69 group, Kappa_score=55%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary - \n",
    "Not sufficient data to build a model with high accuracy and Kappa score.  \n",
    "Most of the classes are unbalanced.  \n",
    "Even after grouping unbalanced classes into large group best accuracy achieved was 69% and kappa_score=55%  \n",
    "Furhter data cleanup, Removal of duplicate words etc improved the score by 1% level only.  \n",
    "Keras deep model with regularization and drop layer improved the scope atleast by 2-3%.  \n",
    "Total 74 iterations are performed with multiple models and tuning parameters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As a next step lets try LDA model to reduce the number of groups so that efficieny can be improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
